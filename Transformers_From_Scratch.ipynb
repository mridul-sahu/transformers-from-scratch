{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM65+G3Ody1ZwFM+xNPvbvc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mridul-sahu/transformers-from-scratch/blob/main/Transformers_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Zero to ¡Hola!: Building a Sassy Spanish Translator with Flax, JAX, and a Dash of Humor\n",
        "\n",
        "¡Hola, intrepid coder! Welcome to an epic quest where we'll transform you from a mere mortal into a bona fide **Neural Machine Translation Maestro**. Or, at the very least, someone who can make a computer utter \"hola\" after seeing \"hello.\" Close enough, right?\n",
        "\n",
        "Our mission, should you choose to accept it (and you kinda already have by being here), is to build an English-to-Spanish translator. But not just *any* translator. We're going to build it using the shiniest, most modern toolkit in the Wild West of AI:\n",
        "\n",
        "* **JAX:** For NumPy-like operations that run on GPUs/TPUs faster than a roadrunner escaping a coyote.\n",
        "* **Flax (NNX):** Our \"LEGO set for neural nets\" – making complex model building feel intuitive and, dare I say, fun! We're using the cutting-edge NNX (Next Generation) API.\n",
        "* **Grain:** The Ferrari of data pipelines. Say goodbye to data bottlenecks and hello to feeding our model at lightning speed.\n",
        "* **Optax:** Our wise sensei for optimization, guiding our model down the path of enlightenment (and lower loss values).\n",
        "* **Orbax:** The diligent lab assistant, ensuring we don't lose our precious model brains to the void (it's for checkpointing, folks).\n",
        "\n",
        "Why this fancy stack? Because we're not just building a model; we're crafting an *experience*. And because these tools are what the cool kids (and serious researchers/engineers) use to push the boundaries of AI. So, grab your favorite beverage, put on your coding hat (the one with the propeller, obviously), and let's turn some Python code into (somewhat) coherent Spanish!"
      ],
      "metadata": {
        "id": "ctLTF3RA_paz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Equipping Our Digital Selves - The Setup Ritual\n",
        "\n",
        "Before we can even dream of our model uttering its first \"hola,\" we need to gather our digital tools. Think of this as the pre-heist montage in a movie, where the team assembles their quirky gadgets. Our gadgets are Python libraries!\n",
        "\n",
        "Here's our shopping list:\n",
        "\n",
        "* `flax`: The star of our show for building neural network architectures with JAX. We'll be using its sleek NNX (Next Generation) API.\n",
        "* `jax` & `jaxlib`: The dynamic duo that enables super-fast numerical computation on accelerators (GPUs/TPUs). `jax` is the frontend, `jaxlib` is the backend muscle.\n",
        "* `optax`: For all our optimization needs – think fancy algorithms to help our model learn efficiently. It's like a personal trainer for your neural net.\n",
        "* `orbax-checkpoint`: Our insurance policy. It saves our model's progress so we don't have to cry if the power goes out mid-training.\n",
        "* `grain`: The high-performance data loading library from Google. It'll make sure our model never goes hungry for data.\n",
        "* `sentencepiece`: The magic word-mincer! It tokenizes our text into bite-sized pieces our model can digest. We'll be using a pre-trained model here.\n",
        "* `tqdm`: Because watching progress bars is the true meaning of life for any programmer training a model.\n",
        "* `numpy`: The granddaddy of numerical Python, still essential for various data manipulations.\n",
        "\n",
        "Let's get 'em!"
      ],
      "metadata": {
        "id": "TNMa3IFP_1VG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IEy1q009W2C"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade \"flax\" \"jax\" \"optax\" \"orbax-checkpoint\" \"grain\" \"sentencepiece\" \"tqdm\" \"clu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let the ancient Pythonic spirits be summoned!\n",
        "# (Or, you know, just import the libraries we need.)\n",
        "\n",
        "import flax\n",
        "from flax import nnx # The shiny new Next Generation Flax API!\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "\n",
        "import numpy as np\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "\n",
        "from grain import python as pygrain\n",
        "\n",
        "import sentencepiece as spm\n",
        "from tqdm.notebook import tqdm # For those lovely progress bars in notebooks\n",
        "\n",
        "# Standard Python libraries\n",
        "import os\n",
        "import pathlib\n",
        "import shutil # For cleaning up directories if needed\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from typing import List, Tuple, Dict, Any, Iterable, Callable, Optional\n",
        "\n",
        "# For reproducibility (optional, but good practice!)\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Flax version: {flax.__version__}\")\n",
        "print(f\"Optax version: {optax.__version__}\")\n",
        "print(f\"Orbax version: {ocp.__version__}\")\n",
        "print(f\"SentencePiece version: {spm.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ],
      "metadata": {
        "id": "sWDIFBhcAtEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Fueling the Machine - Data & Our Trusty Tokenizer\n",
        "\n",
        "Alright, our coding environment is sparkling clean and bristling with new libraries. Now, for the main course: **data**. Our Transformer, much like a very sophisticated Tamagotchi, gets hungry. And it doesn't eat just *any* data. It has a refined palate for tokenized sentences!\n",
        "\n",
        "**The Feast: English-Spanish Sentences**\n",
        "\n",
        "We'll be using a delightful dataset of English sentences and their Spanish translations, lovingly provided by `http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip`. It's a collection of pairs like:\n",
        "\n",
        "* \"Hello.\"  \"Hola.\"\n",
        "* \"I love you.\" \"Te amo.\"\n",
        "* \"My hovercraft is full of eels.\"  \"Mi aerodeslizador está lleno de anguilas.\" (Okay, maybe not that last one, but you get the idea!)\n",
        "\n",
        "**The Magical Word-Mincer: SentencePiece Tokenizer**\n",
        "\n",
        "Raw text is like trying to feed our model a whole watermelon – it just won't work. We need to break it down into manageable, bite-sized pieces called \"tokens.\" For this, we'll use our trusty **SentencePiece tokenizer**, specifically the `eng_spa_spm.model` that we (or someone very kind) trained in a previous adventure (or we'll just download it because we're practical like that). This tokenizer knows the common sub-word units for both English and Spanish in our dataset.\n",
        "\n",
        "Let's go fetch these crucial ingredients!"
      ],
      "metadata": {
        "id": "ozoxgWmuCVn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's set up some paths for our culinary ingredients (data and tokenizer model)\n",
        "data_dir = pathlib.Path(\"/tmp/data\")\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Dataset URL and paths\n",
        "dataset_url = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "zip_file_path = data_dir / \"spa-eng.zip\"\n",
        "extracted_data_path = data_dir / \"spa-eng\"\n",
        "text_file_path = extracted_data_path / \"spa.txt\" # This contains English-Spanish pairs\n",
        "\n",
        "# Tokenizer model URL and path\n",
        "# IMPORTANT: This assumes the tokenizer model is available at this GitHub raw link.\n",
        "# If you have it locally from the previous tutorial, you can adjust the path.\n",
        "tokenizer_model_url = \"https://raw.githubusercontent.com/mridul-sahu/tokenizing-with-sentencepiece/main/eng_spa_spm.model\"\n",
        "tokenizer_model_name = \"eng_spa_spm.model\"\n",
        "tokenizer_model_path = data_dir / tokenizer_model_name\n",
        "\n",
        "# --- Download and Extract Dataset ---\n",
        "if not text_file_path.exists():\n",
        "    if not zip_file_path.exists():\n",
        "        print(f\"Downloading dataset from {dataset_url}...\")\n",
        "        urllib.request.urlretrieve(dataset_url, zip_file_path)\n",
        "        print(f\"Dataset downloaded to {zip_file_path}\")\n",
        "    else:\n",
        "        print(f\"Dataset zip file already exists at {zip_file_path}\")\n",
        "\n",
        "    print(f\"Extracting {zip_file_path}...\")\n",
        "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    print(f\"Dataset extracted to {extracted_data_path}. Expecting {text_file_path.name} inside.\")\n",
        "else:\n",
        "    print(f\"Dataset text file {text_file_path} already exists. Skipping download and extraction.\")\n",
        "\n",
        "# --- Download Tokenizer Model ---\n",
        "if not tokenizer_model_path.exists():\n",
        "    print(f\"Downloading tokenizer model from {tokenizer_model_url}...\")\n",
        "    urllib.request.urlretrieve(tokenizer_model_url, tokenizer_model_path)\n",
        "    print(f\"Tokenizer model downloaded to {tokenizer_model_path}\")\n",
        "else:\n",
        "    print(f\"Tokenizer model {tokenizer_model_path} already exists. Skipping download.\")\n",
        "\n",
        "# Quick check to see if our files are where we expect them\n",
        "assert text_file_path.exists(), f\"Uh oh, {text_file_path} not found! Check download/extraction.\"\n",
        "assert tokenizer_model_path.exists(), f\"Houston, we have a problem: {tokenizer_model_path} not found!\"\n",
        "\n",
        "print(\"\\n--- All files ready! ---\")\n",
        "print(f\"Dataset text file: {text_file_path}\")\n",
        "print(f\"Tokenizer model: {tokenizer_model_path}\")"
      ],
      "metadata": {
        "id": "V26_MTw6Bmvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time to wake up our SentencePiece tokenizer from its slumber!\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(str(tokenizer_model_path)) # .load() expects a string path\n",
        "\n",
        "# Let's see if it understands basic human (and Spanish)\n",
        "sample_english = \"Hello, how are you today?\"\n",
        "sample_spanish = \"Hola, ¿cómo estás hoy?\"\n",
        "\n",
        "print(f\"--- Tokenizer Test ---\")\n",
        "print(f\"Original English: {sample_english}\")\n",
        "print(f\"Tokenized English (pieces): {tokenizer.encode_as_pieces(sample_english)}\")\n",
        "print(f\"Tokenized English (IDs): {tokenizer.encode_as_ids(sample_english)}\")\n",
        "\n",
        "print(f\"\\nOriginal Spanish: {sample_spanish}\")\n",
        "print(f\"Tokenized Spanish (pieces): {tokenizer.encode_as_pieces(sample_spanish)}\")\n",
        "print(f\"Tokenized Spanish (IDs): {tokenizer.encode_as_ids(sample_spanish)}\")\n",
        "\n",
        "# Vocabulary size and special tokens check\n",
        "# These should match what was set during the tokenizer training in the previous tutorial\n",
        "# BOS_ID = 1, EOS_ID = 2, PAD_ID = 0 (or -1 if not explicitly set, but our model used 0)\n",
        "# UNK_ID = 3 (often, check your .vocab file if unsure, or use tokenizer.unk_id())\n",
        "VOCAB_SIZE = tokenizer.get_piece_size()\n",
        "BOS_ID = tokenizer.bos_id() # Typically 1 for SentencePiece\n",
        "EOS_ID = tokenizer.eos_id() # Typically 2 for SentencePiece\n",
        "PAD_ID = tokenizer.pad_id() # Often 0 or -1. Our previous tutorial set it to 0.\n",
        "UNK_ID = tokenizer.unk_id() # Usually 0 if pad is not 0, or 3 if pad,bos,eos are 0,1,2\n",
        "\n",
        "print(f\"\\n--- Tokenizer Properties ---\")\n",
        "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
        "print(f\"BOS ID (<s>): {BOS_ID}\")\n",
        "print(f\"EOS ID (</s>): {EOS_ID}\")\n",
        "print(f\"PAD ID (<pad>): {PAD_ID}\") # Important: Our tokenizer was trained with pad_id=0\n",
        "print(f\"UNK ID (<unk>): {UNK_ID}\")\n",
        "\n",
        "# Let's make sure our PAD_ID is what we expect for padding later.\n",
        "# The previous tutorial explicitly set pad_id=0 during SentencePiece training.\n",
        "# If SentencePiece defaults PAD to -1 and we need 0, this needs careful handling.\n",
        "# Our tokenizer should have PAD_ID = 0 from its training.\n",
        "assert PAD_ID == 0, f\"Expected PAD_ID to be 0, but got {PAD_ID}. This will cause issues with padding!\"\n",
        "# If BOS/EOS are not 1 and 2 respectively, that's also unusual for typical SentencePiece usage.\n",
        "assert BOS_ID == 1, \"BOS ID is not 1!\"\n",
        "assert EOS_ID == 2, \"EOS ID is not 2!\"\n",
        "# UNK_ID is often 0 if PAD is not set, or 3 if PAD/BOS/EOS are 0/1/2.\n",
        "# Our tokenizer was trained with unk_id=3.\n",
        "assert UNK_ID == 3, f\"Expected UNK_ID to be 3, but got {UNK_ID}.\"\n",
        "\n",
        "\n",
        "print(\"\\nOur tokenizer speaks! ¡Y habla! It seems ready for action.\")"
      ],
      "metadata": {
        "id": "u3MrKyCfDIBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Grain - The Ferrari of Data Pipelines (Not Your Average Cereal!)\n",
        "\n",
        "Alright, we have our raw text (the `spa.txt` file) and our trusty tokenizer. Now, how do we get this data into our model efficiently, especially when dealing with potentially massive datasets and the need to keep our hungry GPUs/TPUs fed?\n",
        "\n",
        "Enter **Grain**!\n",
        "\n",
        "Think of Grain as the meticulously organized, hyper-efficient kitchen staff for your Michelin-star restaurant (your model).\n",
        "* It doesn't just load data; it orchestrates it.\n",
        "* It's designed from the ground up for high-performance data loading, especially with JAX and accelerators.\n",
        "* It allows for complex pre-processing pipelines to be defined clearly and executed swiftly.\n",
        "* It handles sharding, shuffling, batching, and prefetching with grace, so your JAX training loop can focus on the number-crunching.\n",
        "\n",
        "Forget those old, clunky data loaders that move like molasses in winter. With Grain, we're building a data pipeline that's more like a Formula 1 pit crew – fast, precise, and ready for action. We'll use it to:\n",
        "\n",
        "1.  Read the English-Spanish pairs from `spa.txt`.\n",
        "2.  Tokenize them using our SentencePiece model.\n",
        "3.  Add special BOS (Beginning Of Sentence) and EOS (End Of Sentence) tokens.\n",
        "4.  Pad sequences to the same length so they can be neatly batched.\n",
        "5.  Batch 'em up!\n",
        "6.  Shuffle them for good measure (keeps the model on its toes!).\n",
        "\n",
        "Let's get our hands dirty and build this data Ferrari!"
      ],
      "metadata": {
        "id": "jHDBx-X9D7Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll set some constants for sequence processing.\n",
        "# You might want to adjust MAX_SEQ_LEN based on your dataset analysis (e.g., 95th percentile length)\n",
        "# For this dataset, many sentences are short, but some can be longer. Let's pick a reasonable starting value.\n",
        "MAX_SEQ_LEN = 64 # Maximum sequence length for padding\n",
        "\n",
        "# --- Step 1: Reading and Parsing the spa.txt file ---\n",
        "# Each line in spa.txt is \"English sentence\\tSpanish sentence\"\n",
        "\n",
        "def load_sentence_pairs(filepath: pathlib.Path) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Loads sentence pairs from the text file.\"\"\"\n",
        "    sentence_pairs = []\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2: # Ensure there's at least English and Spanish\n",
        "                # We might have additional info on some lines (e.g. attribution)\n",
        "                # For this dataset, the first two are consistently English and Spanish\n",
        "                eng_text, spa_text = parts[0], parts[1]\n",
        "                sentence_pairs.append((eng_text, spa_text))\n",
        "    return sentence_pairs\n",
        "\n",
        "# Load all pairs. For a very large dataset, you might process it line by line\n",
        "# or use Grain's FileDataSource, but for spa-eng (around 120k pairs), this is fine.\n",
        "all_sentence_pairs = load_sentence_pairs(text_file_path)\n",
        "\n",
        "# Let's peek at a few to see if we're not just parsing gibberish\n",
        "print(f\"Loaded {len(all_sentence_pairs)} sentence pairs.\")\n",
        "print(\"First 3 pairs:\")\n",
        "for i in range(min(3, len(all_sentence_pairs))):\n",
        "    print(f\"  {all_sentence_pairs[i]}\")"
      ],
      "metadata": {
        "id": "7TKoOqNhDU4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Define Grain Transformations (Tokenizing, Adding Special Tokens) ---\n",
        "\n",
        "# We'll define a transformation that takes a dictionary (one record from our list)\n",
        "# and processes the 'english' and 'spanish' sentences.\n",
        "\n",
        "class ProcessSentencePair(pygrain.MapTransform):\n",
        "    \"\"\"\n",
        "    Grain MapTransform to tokenize and add special BOS/EOS tokens to sentence pairs.\n",
        "    Encoder input: [BOS] ...english_tokens... [EOS]\n",
        "    Decoder input: [BOS] ...spanish_tokens... [EOS]\n",
        "    Targets will be derived from decoder_input_tokens later (e.g., input_tokens[:-1], target_tokens[1:]).\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer: spm.SentencePieceProcessor, max_seq_len: int):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len # This max_seq_len includes BOS and EOS\n",
        "        self.bos_id = self.tokenizer.bos_id()\n",
        "        self.eos_id = self.tokenizer.eos_id()\n",
        "\n",
        "        if self.bos_id is None or self.eos_id is None:\n",
        "            raise ValueError(\"Tokenizer must have BOS and EOS tokens defined.\")\n",
        "\n",
        "    def map(self, features: Tuple[str, str]) -> Dict[str, np.ndarray]:\n",
        "        eng_text = features[0]\n",
        "        spa_text = features[1]\n",
        "\n",
        "        # Tokenize\n",
        "        eng_ids_raw = self.tokenizer.encode_as_ids(eng_text)\n",
        "        spa_ids_raw = self.tokenizer.encode_as_ids(spa_text)\n",
        "\n",
        "        # Prepare encoder input: [BOS] + english_tokens + [EOS]\n",
        "        # Truncate raw tokens to make space for BOS and EOS\n",
        "        # Max raw tokens = max_seq_len - 2\n",
        "        truncated_eng_ids = eng_ids_raw[:self.max_seq_len - 2]\n",
        "        encoder_input_ids = [self.bos_id] + truncated_eng_ids + [self.eos_id]\n",
        "\n",
        "        # Prepare decoder \"input/target\" tokens: [BOS] + spanish_tokens + [EOS]\n",
        "        # Raw Spanish tokens also truncated to make space for BOS and EOS\n",
        "        truncated_spa_ids = spa_ids_raw[:self.max_seq_len - 2]\n",
        "        # This will be used to derive model's decoder input and the target for loss later\n",
        "        decoder_full_sequence_ids = [self.bos_id] + truncated_spa_ids + [self.eos_id]\n",
        "\n",
        "        return {\n",
        "            \"encoder_input_tokens\": encoder_input_ids,\n",
        "            \"decoder_input_tokens\": decoder_full_sequence_ids,\n",
        "        }\n",
        "\n",
        "# Instantiate our transformation\n",
        "# We're proceeding with MAX_SEQ_LEN = 64 for the tutorial for speed, as per user's last decision.\n",
        "# In a real scenario, MAX_SEQ_LEN = 128 or based on analysis would be better.\n",
        "sentence_processor = ProcessSentencePair(tokenizer=tokenizer, max_seq_len=MAX_SEQ_LEN)"
      ],
      "metadata": {
        "id": "PcKlCcYFFtYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Padding & Batching with Grain ---\n",
        "# Our sentences, processed by the ProcessSentencePair, will have varying lengths\n",
        "# up to MAX_SEQ_LEN (which is 64 for this tutorial run).\n",
        "# Neural nets love their data in neat, uniform tensors. So, we pad!\n",
        "\n",
        "# We'll use the MAX_SEQ_LEN and PAD_ID (which should be 0).\n",
        "\n",
        "# Define features that need padding and their target length (MAX_SEQ_LEN).\n",
        "# This is now simpler as we only have two token features from ProcessSentencePair.\n",
        "feature_lengths_to_pad = {\n",
        "    \"encoder_input_tokens\": MAX_SEQ_LEN,\n",
        "    \"decoder_input_tokens\": MAX_SEQ_LEN,\n",
        "}\n",
        "\n",
        "class PadToMaxLength(pygrain.MapTransform):\n",
        "    \"\"\"Pads the token sequences to MAX_SEQ_LEN.\"\"\"\n",
        "    def __init__(self, feature_lengths: Dict[str, int], pad_id: int):\n",
        "        self._feature_lengths = feature_lengths\n",
        "        self._pad_id = pad_id\n",
        "\n",
        "    def map(self, features: Dict[str, list]) -> Dict[str, np.ndarray]:\n",
        "        padded_features = {}\n",
        "        for k, target_len in self._feature_lengths.items():\n",
        "            if k not in features:\n",
        "                # This might happen if a feature is unexpectedly missing\n",
        "                raise ValueError(f\"Expected feature {k} not found in input to PadToMaxLength.\")\n",
        "\n",
        "            v = features[k]\n",
        "\n",
        "            padding_amount = target_len - len(v)\n",
        "            if padding_amount < 0:\n",
        "                # This implies ProcessSentencePair produced a sequence longer than MAX_SEQ_LEN\n",
        "                raise ValueError(\n",
        "                    f\"Feature {k} has length {v.shape[0]}, which is > MAX_SEQ_LEN {target_len} \"\n",
        "                    f\"before padding. Check ProcessSentencePair truncation.\"\n",
        "                )\n",
        "            padded_features[k] = np.pad(\n",
        "                v,\n",
        "                pad_width=((0, padding_amount)), # Pad at the end\n",
        "                mode='constant',\n",
        "                constant_values=self._pad_id\n",
        "            )\n",
        "        # Copy over any other features that might exist and don't need padding\n",
        "        # (though our current ProcessSentencePair only returns the ones we pad)\n",
        "        for k_orig, v_orig in features.items():\n",
        "            if k_orig not in padded_features:\n",
        "                padded_features[k_orig] = v_orig\n",
        "        return padded_features\n",
        "\n",
        "padder = PadToMaxLength(feature_lengths=feature_lengths_to_pad, pad_id=PAD_ID)\n",
        "\n",
        "# Now for Batching!\n",
        "BATCH_SIZE = 64\n",
        "batch_op = pygrain.Batch(batch_size=BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "N1qahuUTIkpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: The Grand Grain Finale - DataLoaders! ) ---\n",
        "\n",
        "NUM_EXAMPLES_CAP = len(all_sentence_pairs) # Using a subset for faster tutorial execution\n",
        "\n",
        "def get_dataset_iterators(num_examples_cap=NUM_EXAMPLES_CAP, num_epochs=1):\n",
        "  # Operations for the DataLoader, reflecting the pipeline:\n",
        "  # 1. Process sentences (tokenize, add BOS/EOS to both encoder and decoder inputs)\n",
        "  # 2. Pad to MAX_SEQ_LEN (only encoder_input_tokens and decoder_input_tokens)\n",
        "  # 3. Batch\n",
        "  operations = [\n",
        "      sentence_processor, # Outputs 'encoder_input_tokens', 'decoder_input_tokens'\n",
        "      padder,             # Pads these two features\n",
        "      batch_op            # Batches the padded features\n",
        "  ]\n",
        "  if len(all_sentence_pairs) > NUM_EXAMPLES_CAP:\n",
        "      print(f\"\\nINFO: Capping dataset to {NUM_EXAMPLES_CAP} examples for this tutorial run.\")\n",
        "      train_subset = all_sentence_pairs[:NUM_EXAMPLES_CAP]\n",
        "      eval_subset = all_sentence_pairs[NUM_EXAMPLES_CAP:]\n",
        "  else:\n",
        "      train_subset = all_sentence_pairs\n",
        "      eval_subset = all_sentence_pairs\n",
        "\n",
        "  # Create the training DataLoader\n",
        "  train_data_loader = pygrain.DataLoader(\n",
        "      data_source=train_subset,\n",
        "      sampler=pygrain.IndexSampler(\n",
        "          num_records=len(train_subset),\n",
        "          num_epochs=num_epochs),\n",
        "      operations=operations,\n",
        "  )\n",
        "  # Create the eval DataLoader\n",
        "  eval_data_loader = pygrain.DataLoader(\n",
        "      data_source=eval_subset,\n",
        "      sampler=pygrain.IndexSampler(\n",
        "          num_records=len(eval_subset),\n",
        "          num_epochs=1),\n",
        "      operations=operations,\n",
        "  )\n",
        "  return train_data_loader, eval_data_loader\n",
        "\n",
        "train_data_loader, eval_data_loader = get_dataset_iterators()\n",
        "print(\"PyGrain DataLoaders for training and eval are set up!\")\n",
        "\n",
        "# Let's grab one batch and inspect its (now simpler) divine structure.\n",
        "print(\"\\n--- Inspecting a Sample Batch from DataLoader ---\")\n",
        "try:\n",
        "    example_batch = next(iter(train_data_loader))\n",
        "    print(\"Successfully fetched a train batch!\")\n",
        "    for name, array_data in example_batch.items():\n",
        "        print(f\"  Feature: {name}, Shape: {array_data.shape}, Dtype: {array_data.dtype}\")\n",
        "\n",
        "    # Let's decode one example from the batch\n",
        "    print(\"\\n  Decoding first example from the batch:\")\n",
        "    idx_in_batch = 0\n",
        "    encoder_tokens = example_batch['encoder_input_tokens'][idx_in_batch]\n",
        "    decoder_tokens = example_batch['decoder_input_tokens'][idx_in_batch]\n",
        "\n",
        "    print(f\"    Encoder Input (tokens): {tokenizer.decode_ids(encoder_tokens.tolist())}\")\n",
        "    print(f\"    Decoder Sequence (tokens, for deriving input/target): {tokenizer.decode_ids(decoder_tokens.tolist())}\")\n",
        "\n",
        "    example_batch = next(iter(eval_data_loader))\n",
        "    print(\"Successfully fetched a eval batch!\")\n",
        "    for name, array_data in example_batch.items():\n",
        "        print(f\"  Feature: {name}, Shape: {array_data.shape}, Dtype: {array_data.dtype}\")\n",
        "\n",
        "    # Let's decode one example from the batch\n",
        "    print(\"\\n  Decoding first example from the batch:\")\n",
        "    idx_in_batch = 0\n",
        "    encoder_tokens = example_batch['encoder_input_tokens'][idx_in_batch]\n",
        "    decoder_tokens = example_batch['decoder_input_tokens'][idx_in_batch]\n",
        "\n",
        "    print(f\"    Encoder Input (tokens): {tokenizer.decode_ids(encoder_tokens.tolist())}\")\n",
        "    print(f\"    Decoder Sequence (tokens, for deriving input/target): {tokenizer.decode_ids(decoder_tokens.tolist())}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not get a batch from the DataLoader. Error: {e}\")\n",
        "    print(\"Ensure 'processed_records_for_grain' is not empty and all previous Grain setup cells ran correctly.\")"
      ],
      "metadata": {
        "id": "oyccDc_SLbUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Building Our Transformer: Less Scary Than IKEA Furniture (Promise!)\n",
        "\n",
        "Phew! Our data is preened, polished, padded, and presented in perfect batches by our Grain-powered pipeline. It's like we've prepared a gourmet seven-course meal for a king. And who is this king? Our **Transformer model**, of course!\n",
        "\n",
        "Now, the word \"Transformer\" might conjure images of giant battling robots or fiendishly complex diagrams with arrows flying everywhere. And well, the diagrams aren't entirely wrong, but don't panic! We're going to break it down piece by piece.\n",
        "\n",
        "**The Blueprint: Attention is All You Need (and a few other bits)**\n",
        "\n",
        "At its heart, a Seq2Seq (Sequence-to-Sequence) Transformer for translation has two main parts:\n",
        "\n",
        "1.  **The Encoder:** Reads the input sentence (e.g., English) and builds a rich representation of it. Think of it as understanding the *meaning* and *context* of the input.\n",
        "2.  **The Decoder:** Takes that representation from the Encoder and, step-by-step, generates the output sentence in the target language (e.g., Spanish). It's a bit like a writer crafting a sentence, looking back at their notes (the Encoder's output) as they go.\n",
        "\n",
        "The magic ingredient that makes Transformers so powerful is **attention** (specifically, \"self-attention\" and \"cross-attention\"). It allows the model to weigh the importance of different words in the input when processing a particular word, and for the Decoder to focus on relevant parts of the input sentence when generating an output word.\n",
        "\n",
        "**Our Toolkit: Flax NNX - LEGOs for Neural Networks!**\n",
        "\n",
        "To build this magnificent beast, we'll use **Flax NNX**. NNX is the \"Next Generation\" API for Flax, designed to be even more Pythonic, flexible, and easier to work with when defining complex models in JAX. It makes defining layers and modules feel like snapping together high-tech LEGO bricks. Each \"brick\" (like an attention mechanism or a feed-forward layer) will be a self-contained NNX `Module`.\n",
        "\n",
        "Let's start assembling! First up: giving our words a sense of their position in the sentence."
      ],
      "metadata": {
        "id": "Re7CMufYRobj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Block 1: Positional & Token Embeddings - Giving Words Identity and a GPS!\n",
        "\n",
        "Alright, picture our Transformer. It's about to read a sentence. If it were a human, it would naturally understand that \"The cat chased the dog\" is different from \"The dog chased the cat.\" Word order matters!\n",
        "\n",
        "But our core Transformer layers (like attention, which we'll meet soon) are a bit like a bag-of-words model initially – they're great at seeing *which* words are there, but not so much *where* they are in the sequence. Without help, \"cat chased dog\" and \"dog chased cat\" might look confusingly similar to some internal parts of the model.\n",
        "\n",
        "To fix this, we need to give our model a sense of **position**. We need to embed not just the meaning of each word (token embedding) but also its location in the sentence.\n",
        "\n",
        "There are a few ways to do this:\n",
        "* **Fixed Positional Encodings:** Some methods use clever mathematical functions (like sines and cosines of different frequencies) to create a unique signal for each position. These are pre-calculated and don't change during training.\n",
        "* **Learnable Positional Embeddings:** Another popular approach is to let the model *learn* an embedding vector for each position, just like it learns an embedding vector for each word in its vocabulary. This can offer more flexibility.\n",
        "\n",
        "**Our Approach:** We're going to be extra efficient and a bit fancy! We'll create a single module that does two crucial jobs:\n",
        "1.  **Token Embedding:** Converts our input token IDs (just numbers representing words/sub-words) into rich vector representations.\n",
        "2.  **Learnable Positional Embedding:** Creates learnable vector representations for each position in the sequence.\n",
        "\n",
        "Then, it simply adds these two embeddings together for each token. So, the vector that the rest of the Transformer sees for each token will contain information about both *what* the token is and *where* it is. How neat is that?\n",
        "\n",
        "Let's build this `PositionalEmbedding` powerhouse using Flax NNX!"
      ],
      "metadata": {
        "id": "l1186_WORvZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transformer Building Block (Revised): Learnable Positional Embedding ---\n",
        "\n",
        "class PositionalEmbedding(nnx.Module):\n",
        "    \"\"\"\n",
        "    Combines token embedding and learnable positional embedding.\n",
        "    Takes token IDs as input.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 max_sequence_length: int,\n",
        "                 vocab_size: int,\n",
        "                 embed_dim: int, # This is in_features for the transformer\n",
        "                 *,\n",
        "                 rngs: nnx.Rngs # Expects {'params': key_for_params}\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_sequence_length: The maximum possible sequence length for positional embeddings.\n",
        "            vocab_size: The size of the token vocabulary.\n",
        "            embed_dim: The dimensionality of the output embeddings.\n",
        "            rngs: NNX Rngs dictionary, primarily for 'params' PRNG key.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Token embedding layer\n",
        "        # nnx.Embed uses the 'params' RNG stream from the rngs dict for its kernel initialization\n",
        "        self.token_embedder = nnx.Embed(\n",
        "            num_embeddings=self.vocab_size,\n",
        "            features=self.embed_dim,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "\n",
        "        # Learnable positional embedding layer\n",
        "        self.position_embedder = nnx.Embed(\n",
        "            num_embeddings=self.max_sequence_length,\n",
        "            features=self.embed_dim,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "\n",
        "    def __call__(self, input_token_ids: jax.Array) -> jax.Array:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_token_ids: A JAX array of token IDs.\n",
        "                             Shape: (batch_size, sequence_length)\n",
        "        Returns:\n",
        "            Combined token and positional embeddings.\n",
        "            Shape: (batch_size, sequence_length, embed_dim)\n",
        "        \"\"\"\n",
        "        batch_size, actual_sequence_length = input_token_ids.shape\n",
        "\n",
        "        if actual_sequence_length > self.max_sequence_length:\n",
        "            raise ValueError(\n",
        "                f\"Input sequence length ({actual_sequence_length}) exceeds \"\n",
        "                f\"PositionalEmbedding's configured max_sequence_length ({self.max_sequence_length}).\"\n",
        "            )\n",
        "\n",
        "        # 1. Get token embeddings\n",
        "        # Input shape: (batch_size, sequence_length)\n",
        "        # Output shape: (batch_size, sequence_length, embed_dim)\n",
        "        token_embeddings_output = self.token_embedder(input_token_ids)\n",
        "\n",
        "        # 2. Get positional embeddings\n",
        "        # Create position indices: [0, 1, ..., actual_sequence_length-1]\n",
        "        positions = jnp.arange(actual_sequence_length, dtype=jnp.int32)\n",
        "        # The position_embedder expects an input that it can map.\n",
        "        # If we pass `positions` of shape (actual_sequence_length,),\n",
        "        # `nnx.Embed` will output shape (actual_sequence_length, embed_dim).\n",
        "        # This then needs to be broadcastable to the token_embeddings_output shape.\n",
        "        position_embeddings_output = self.position_embedder(positions) # Shape: (seq_len, embed_dim)\n",
        "        # Add a batch dimension for broadcasting: (1, seq_len, embed_dim)\n",
        "        position_embeddings_output = position_embeddings_output[jnp.newaxis, :, :]\n",
        "\n",
        "        # 3. Add them together\n",
        "        # (batch_size, sequence_length, embed_dim) + (1, sequence_length, embed_dim)\n",
        "        # -> (batch_size, sequence_length, embed_dim) due to broadcasting\n",
        "        return token_embeddings_output + position_embeddings_output"
      ],
      "metadata": {
        "id": "s8ENzumdMVgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Block 2: Multi-Head Attention - The Art of Focused Eavesdropping\n",
        "\n",
        "So, our tokens are now nicely embedded, carrying information about both *what* they are and *where* they are (thanks to our `PositionalEmbedding` layer). What's next? It's time for them to start talking to each other! This is where **Attention** mechanisms come in, and specifically, **Multi-Head Attention**.\n",
        "\n",
        "Imagine you're at a noisy party (our sequence of words) trying to understand what one person (a \"query\" word) is saying.\n",
        "* You'd naturally focus more on that person and perhaps on others who are relevant to their conversation (these are \"key\" words).\n",
        "* The actual information you glean comes from what these relevant people are saying (their \"value\" words).\n",
        "\n",
        "**Self-Attention** is when the \"query,\" \"key,\" and \"value\" all come from the *same* sequence of words. Each word looks at all other words in the sentence (including itself) to figure out which ones are most relevant to it, and then uses that to update its own representation. It's like every word in the sentence having a sophisticated internal discussion to understand its context better.\n",
        "\n",
        "**Why \"Multi-Head\"?**\n",
        "Instead of doing this attention calculation just once, Multi-Head Attention does it multiple times in parallel, each with slightly different learned \"perspectives\" (these are the \"heads\").\n",
        "* Think of it as having multiple people at the party, each listening for different aspects of the conversation. One head might focus on syntactic relationships, another on semantic ones, and so on.\n",
        "* Each \"head\" produces its own attention output. These outputs are then combined (concatenated and linearly projected) to get a final, richer representation.\n",
        "\n",
        "This allows the model to jointly attend to information from different representation subspaces at different positions. Sounds fancy, right? It basically means the model gets a more nuanced understanding by looking at the input from various angles simultaneously.\n",
        "\n",
        "We'll be using the native `flax.nnx.MultiHeadAttention` for this, which is a powerful, configurable, and optimized implementation. It handles the projections, head splitting, scaled dot-product attention, and combining heads internally."
      ],
      "metadata": {
        "id": "2UOMaLz0f8Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Block 3: Position-wise Feed-Forward Network (FFN) - The Transformer's Personal Trainer\n",
        "\n",
        "Our tokens have gone through the intense social mixer of Multi-Head Attention, gathering context from their peers. What now? They need some private processing time, a bit like hitting the gym after a conference to digest all the information.\n",
        "\n",
        "This is where the **Position-wise Feed-Forward Network (FFN)** comes in.\n",
        "It's a relatively simple component, but crucial. For each position in the sequence *independently*, the FFN applies the *same* two linear transformations with an activation function in between.\n",
        "\n",
        "* **\"Position-wise\"** means it processes each token's representation separately from other tokens. The token at position `i` gets processed without directly looking at the token at position `j` *within this FFN step*. (They've already \"talked\" in the attention step).\n",
        "* **\"Feed-Forward Network\"** typically means a couple of fully connected layers. A common setup is:\n",
        "    1.  A linear transformation that expands the dimensionality (e.g., from `in_features` to `hidden_dim`, where `hidden_dim` is often `4 * in_features`).\n",
        "    2.  An activation function (like ReLU or GeLU).\n",
        "    3.  Another linear transformation that projects it back down to `in_features`.\n",
        "\n",
        "This FFN allows the model to perform more complex transformations on each token's representation, adding more learning capacity. Think of it as adding depth to the processing at each position.\n",
        "\n",
        "Let's build this straightforward but mighty module!"
      ],
      "metadata": {
        "id": "LGEOKkFWgGu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transformer Building Block: Position-wise Feed-Forward Network ---\n",
        "\n",
        "class FeedForward(nnx.Module):\n",
        "    \"\"\"\n",
        "    A position-wise feed-forward network.\n",
        "    Consists of two linear layers with an activation in between.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_features: int,      # Input and output dimensionality\n",
        "                 hidden_dim: int,       # Inner dimensionality (often 4 * in_features)\n",
        "                 dropout_rate: float = 0.1, # Dropout after the activation\n",
        "                 activation_fn: Callable = nnx.relu, # Or nn.gelu or jax.nn.gelu\n",
        "                 *,\n",
        "                 rngs: nnx.Rngs,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "        # Use nnx.Linear for the dense layers\n",
        "        self.linear_1 = nnx.Linear(in_features=in_features, out_features=hidden_dim, rngs=rngs)\n",
        "        self.linear_2 = nnx.Linear(in_features=hidden_dim, out_features=in_features, rngs=rngs)\n",
        "\n",
        "        if self.dropout_rate > 0:\n",
        "            # nnx.Dropout for applying dropout\n",
        "            self.dropout_layer = nnx.Dropout(rate=self.dropout_rate, rngs=rngs) # Uses 'dropout' stream from rngs\n",
        "\n",
        "    def __call__(self, x: jax.Array) -> jax.Array:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor. Shape: (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            Output tensor. Shape: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        x = self.linear_1(x)\n",
        "        x = self.activation_fn(x)\n",
        "        if self.dropout_rate > 0:\n",
        "            # Pass deterministic flag to nnx.Dropout's call method\n",
        "            x = self.dropout_layer(x)\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SuqvaDK2Y3Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Block 4: The Encoder Layer - Assembling the \"Understanding\" Unit\n",
        "\n",
        "We've now built (or rather, decided to use the official version of) two of the most critical components of a Transformer layer:\n",
        "1.  **Multi-Head Attention (`flax.nnx.MultiHeadAttention`):** Allows tokens to look at and incorporate information from other tokens in the sequence.\n",
        "2.  **Position-wise Feed-Forward Network (`FeedForward`):** Applies a non-linear transformation to each token's representation independently.\n",
        "\n",
        "An **Encoder Layer** in a Transformer simply combines these two sub-layers. But there's a bit more to it than just sticking them together. To help with training deep networks (and Transformers can get very deep!), we also use:\n",
        "\n",
        "1.  **Residual Connections (Add):** For each sub-layer (Attention and FFN), the input to that sub-layer is added to its output. This is like saying, \"Okay, sub-layer, do your fancy transformation, but also remember what things were like before you started.\" This helps gradients flow better during backpropagation and prevents information from being lost too quickly. `output = SubLayer(input) + input`\n",
        "2.  **Layer Normalization (Norm):** Applied *before* each sub-layer (in the \"pre-LN\" variant, which is common and often more stable) or *after* the residual addition (in the \"post-LN\" variant from the original paper). Layer Normalization helps stabilize the activations and gradients, making training smoother. It normalizes the features for each token independently across its embedding dimension. We'll likely use `flax.nnx.RMSNorm`.\n",
        "\n",
        "So, a typical Pre-LN Encoder Layer looks like this:\n",
        "\n",
        "1.  Input `x`\n",
        "2.  `x_norm1 = LayerNorm(x)`\n",
        "3.  `attention_out = MultiHeadSelfAttention(query=x_norm1, key=x_norm1, value=x_norm1, ...)`\n",
        "4.  `x_residual1 = x + Dropout(attention_out)` (Dropout is often applied to the output of sub-layers before adding to residual)\n",
        "5.  `x_norm2 = LayerNorm(x_residual1)`\n",
        "6.  `ffn_out = FeedForwardNetwork(x_norm2)`\n",
        "7.  `output = x_residual1 + Dropout(ffn_out)`\n",
        "\n",
        "The Encoder itself will then be a stack of these Encoder Layers. Each layer refines the representation of the input sequence, hopefully leading to a rich, contextual understanding. Let's sketch out this Encoder Layer module!"
      ],
      "metadata": {
        "id": "7lYeabVLjT0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transformer Building Block: Encoder Layer ---\n",
        "# This layer combines Multi-Head Attention and our FeedForward network,\n",
        "# with residual connections, layer normalization, and dropout.\n",
        "\n",
        "class EncoderLayer(nnx.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 num_heads: int,\n",
        "                 hidden_dim: int, # Hidden dimension for the FeedForward network\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 activation_fn: Callable = jax.nn.relu, # e.g., jax.nn.relu or jax.nn.gelu\n",
        "                 *,\n",
        "                 rngs: nnx.Rngs,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Self-Attention block\n",
        "        # Assuming nnx.MultiHeadAttention definition from user-provided source is available\n",
        "        self.self_attention = nnx.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            in_features=in_features, # in_features is the input feature size\n",
        "            # qkv_features & out_features default to in_features in nnx.MultiHeadAttention\n",
        "            dropout_rate=dropout_rate, # MHA has its own internal dropout for attention weights\n",
        "            decode=False,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.norm1 = nnx.RMSNorm(num_features=in_features, rngs=rngs)\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout1 = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
        "\n",
        "        # FeedForward block\n",
        "        # Using the user-refined FeedForward class structure\n",
        "        self.feed_forward = FeedForward(\n",
        "            in_features=in_features,\n",
        "            hidden_dim=hidden_dim,\n",
        "            dropout_rate=dropout_rate,\n",
        "            activation_fn=activation_fn,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.norm2 = nnx.RMSNorm(num_features=in_features, rngs=rngs)\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout2 = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jax.Array, attention_mask: Optional[jax.Array]) -> jax.Array:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input to the encoder layer. Shape: (batch_size, seq_len, in_features)\n",
        "            attention_mask: Boolean mask for self-attention.\n",
        "                            True where valid. Shape: (batch_size, 1, 1, seq_len) or broadcastable.\n",
        "        Returns:\n",
        "            Output of the encoder layer. Shape: (batch_size, seq_len, in_features)\n",
        "        \"\"\"\n",
        "        # 1. Self-Attention sub-layer (Pre-LN)\n",
        "        x_norm1 = self.norm1(x)\n",
        "        # nnx.MultiHeadAttention returns only the output, not attention weights\n",
        "        # For self-attention, query, key, and value are all derived from x_norm1\n",
        "        attention_output = self.self_attention(\n",
        "            inputs_q=x_norm1,\n",
        "            inputs_k=x_norm1, # Self-attention\n",
        "            inputs_v=x_norm1, # Self-attention\n",
        "            mask=attention_mask,\n",
        "        )\n",
        "        if self.dropout_rate > 0:\n",
        "            attention_output = self.dropout1(attention_output)\n",
        "        x = x + attention_output # Residual connection\n",
        "\n",
        "        # 2. Feed-Forward sub-layer (Pre-LN)\n",
        "        x_norm2 = self.norm2(x)\n",
        "        ffn_output = self.feed_forward(x_norm2)\n",
        "        if self.dropout_rate > 0: # if we want another dropout here\n",
        "             ffn_output_after_dropout = self.dropout2(ffn_output)\n",
        "             x = x + ffn_output_after_dropout\n",
        "        else:\n",
        "             x = x + ffn_output # Residual connection\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UE0RkoJMij4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Block 5: The Encoder - Stacking Layers for Deeper Understanding\n",
        "\n",
        "We've successfully built an `EncoderLayer`! This layer takes a sequence of embeddings, applies self-attention to allow tokens to interact, and then uses a feed-forward network to further process each token. It's a pretty neat package of computation.\n",
        "\n",
        "But one layer is often not enough to capture all the complex patterns and nuances in language. The original Transformer paper, and many successful models since, stack multiple identical `EncoderLayer` instances on top of each other to form the complete **Encoder**.\n",
        "\n",
        "Think of it like this:\n",
        "* The first `EncoderLayer` might capture some local contextual information.\n",
        "* The second layer takes the output of the first (which is now a more context-aware representation) and processes it further, perhaps capturing longer-range dependencies or more abstract features.\n",
        "* Each subsequent layer builds upon the representations from the layer below it.\n",
        "\n",
        "By stacking these layers (e.g., N=6 in the original paper, but it can be 2, 4, 8, 12, or even more), the Encoder can progressively build richer and more complex representations of the input sentence. The output of the final `EncoderLayer` in the stack is then the \"meaning vector\" or \"contextual representation\" that gets passed to the Decoder.\n",
        "\n",
        "Our `Encoder` module will therefore be quite simple: it will primarily be a list or sequence of `EncoderLayer` instances, and its `__call__` method will just pass the input data through these layers sequentially. We might also add a final `LayerNorm` at the very end of the Encoder stack, which is sometimes done."
      ],
      "metadata": {
        "id": "6IM8SVManX7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transformer Building Block: Full Encoder ---\n",
        "\n",
        "class Encoder(nnx.Module):\n",
        "    \"\"\"\n",
        "    The full Encoder stack, composed of multiple EncoderLayers.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_layers: int,\n",
        "                 in_features: int,\n",
        "                 num_heads: int,\n",
        "                 hidden_dim: int,\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 activation_fn: Callable = jax.nn.relu,\n",
        "                 *,\n",
        "                 rngs: nnx.Rngs,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.in_features = in_features\n",
        "\n",
        "        self.layers = [\n",
        "            EncoderLayer(\n",
        "                in_features=in_features,\n",
        "                num_heads=num_heads,\n",
        "                hidden_dim=hidden_dim,\n",
        "                dropout_rate=dropout_rate,\n",
        "                activation_fn=activation_fn,\n",
        "                rngs=rngs,\n",
        "            ) for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.final_norm = nnx.RMSNorm(num_features=in_features, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jax.Array, attention_mask: Optional[jax.Array]) -> jax.Array:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input to the encoder. Shape: (batch_size, seq_len, in_features)\n",
        "               (This is typically the output of the PositionalEmbedding layer)\n",
        "            attention_mask: Boolean mask for self-attention in all layers.\n",
        "        Returns:\n",
        "            Output of the encoder. Shape: (batch_size, seq_len, in_features)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask=attention_mask)\n",
        "\n",
        "        x = self.final_norm(x) # Apply final normalization\n",
        "        return x"
      ],
      "metadata": {
        "id": "WcVH2fZwlupH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Block 6: The Decoder Layer - Crafting the Output, One Word at a Time\n",
        "\n",
        "Alright, our Encoder is busy understanding the input sentence and producing a rich set of contextual embeddings for it. Now, we need the **Decoder** to take these encoder outputs and generate the translated sentence in the target language, token by token.\n",
        "\n",
        "A **Decoder Layer** is structurally similar to an `EncoderLayer` but with a crucial addition:\n",
        "\n",
        "1.  **Masked Self-Attention (First Sub-layer):**\n",
        "    * The Decoder is autoregressive, meaning it generates the output sequence one token at a time. When predicting the token at position `i`, it should only be allowed to attend to tokens at positions less than or equal to `i` in the output sequence generated so far. It shouldn't peek at future tokens!\n",
        "    * This is achieved by applying a **causal mask** (or look-ahead mask) to the self-attention mechanism. This mask effectively hides future positions.\n",
        "    * Otherwise, it's similar to the self-attention in the Encoder Layer: LayerNorm -> Masked MultiHeadSelfAttention -> Dropout -> Add.\n",
        "\n",
        "2.  **Encoder-Decoder Attention (Cross-Attention - Second Sub-layer):**\n",
        "    * This is where the Decoder \"looks at\" the output of the Encoder.\n",
        "    * The **queries** for this attention mechanism come from the output of the Decoder's first sub-layer (the masked self-attention).\n",
        "    * The **keys and values** come from the final output of the *Encoder stack*.\n",
        "    * This allows each token being generated by the Decoder to attend to all tokens in the *input sentence* (via the Encoder's output) to decide what information is most relevant for predicting the current output token.\n",
        "    * It also uses LayerNorm -> MultiHeadAttention -> Dropout -> Add. The mask here would be the padding mask from the *encoder's input*.\n",
        "\n",
        "3.  **Feed-Forward Network (Third Sub-layer):**\n",
        "    * Identical in structure to the FFN in the `EncoderLayer` (LayerNorm -> FFN -> Dropout -> Add). It processes the output from the Encoder-Decoder attention.\n",
        "\n",
        "So, a Decoder Layer has *three* main sub-components instead of two. It needs to handle two types of attention masks: one for its own causal self-attention and one for attending to the (potentially padded) encoder output.\n",
        "\n",
        "This layer is the workhorse of the generation process!"
      ],
      "metadata": {
        "id": "ndjFDpibpSvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transformer Building Block: Decoder Layer ---\n",
        "\n",
        "class DecoderLayer(nnx.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 num_heads: int,\n",
        "                 hidden_dim: int,\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 activation_fn: Callable = jax.nn.relu,\n",
        "                 *,\n",
        "                 rngs: nnx.Rngs\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # 1. Masked Self-Attention block\n",
        "        self.masked_self_attention = nnx.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            in_features=in_features,\n",
        "            dropout_rate=dropout_rate,\n",
        "            decode=False,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.norm1 = nnx.RMSNorm(num_features=in_features, rngs=rngs)\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout1 = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
        "\n",
        "        # 2. Encoder-Decoder Attention (Cross-Attention) block\n",
        "        self.encoder_decoder_attention = nnx.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            in_features=in_features,\n",
        "            dropout_rate=dropout_rate,\n",
        "            decode=False,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.norm2 = nnx.RMSNorm(num_features=in_features, rngs=rngs)\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout2 = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
        "\n",
        "        # 3. FeedForward block\n",
        "        self.feed_forward = FeedForward(\n",
        "            in_features=in_features,\n",
        "            hidden_dim=hidden_dim,\n",
        "            dropout_rate=dropout_rate,\n",
        "            activation_fn=activation_fn,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.norm3 = nnx.RMSNorm(num_features=in_features, rngs=rngs)\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout3 = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
        "\n",
        "    def __call__(self,\n",
        "                 x: jax.Array, # Target sequence embeddings (from PositionalEmbedding)\n",
        "                 encoder_output: jax.Array, # Output from the Encoder stack\n",
        "                 causal_mask: Optional[jax.Array], # For masked self-attention (target sequence)\n",
        "                 encoder_attention_mask: Optional[jax.Array], # For encoder-decoder attention (padding in source sequence)\n",
        "                ) -> jax.Array:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input to the decoder layer. Shape: (batch_size, target_seq_len, in_features)\n",
        "            encoder_output: Output from the encoder. Shape: (batch_size, source_seq_len, in_features)\n",
        "            causal_mask: Boolean mask for masked self-attention. Prevents attending to future target tokens.\n",
        "                         Shape typically (batch_size, 1, target_seq_len, target_seq_len).\n",
        "            encoder_attention_mask: Boolean mask for encoder-decoder attention. Hides padding in encoder output.\n",
        "                                    Shape typically (batch_size, 1, 1, source_seq_len).\n",
        "        Returns:\n",
        "            Output of the decoder layer. Shape: (batch_size, target_seq_len, in_features)\n",
        "        \"\"\"\n",
        "        # 1. Masked Self-Attention (target sequence queries itself)\n",
        "        x_norm1 = self.norm1(x)\n",
        "        # Query, Key, Value are all from x_norm1 for self-attention\n",
        "        self_attn_output = self.masked_self_attention(\n",
        "            inputs_q=x_norm1,\n",
        "            inputs_k=x_norm1,\n",
        "            inputs_v=x_norm1,\n",
        "            mask=causal_mask,\n",
        "        )\n",
        "        if self.dropout_rate > 0:\n",
        "            self_attn_output = self.dropout1(self_attn_output)\n",
        "        x = x + self_attn_output # Residual\n",
        "\n",
        "        # 2. Encoder-Decoder Attention (target sequence queries encoder output)\n",
        "        x_norm2 = self.norm2(x)\n",
        "        # Query from decoder's previous output (x_norm2), Key & Value from encoder_output\n",
        "        cross_attn_output = self.encoder_decoder_attention(\n",
        "            inputs_q=x_norm2,\n",
        "            inputs_k=encoder_output,\n",
        "            inputs_v=encoder_output,\n",
        "            mask=encoder_attention_mask, # Mask based on encoder padding\n",
        "        )\n",
        "        if self.dropout_rate > 0:\n",
        "            cross_attn_output = self.dropout2(cross_attn_output)\n",
        "        x = x + cross_attn_output # Residual\n",
        "\n",
        "        # 3. Feed-Forward Network\n",
        "        x_norm3 = self.norm3(x)\n",
        "        ffn_output = self.feed_forward(x_norm3)\n",
        "        if self.dropout_rate > 0:\n",
        "            ffn_output_after_dropout = self.dropout3(ffn_output)\n",
        "            x = x + ffn_output_after_dropout\n",
        "        else:\n",
        "            x = x + ffn_output # Residual\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Hhqdn0eDorzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Block 7: The Decoder - Stacking Up Layers of Generation Power\n",
        "\n",
        "Just like our Encoder, the **Decoder** in a Transformer is typically composed of a stack of identical `DecoderLayer` instances. We've already crafted the `DecoderLayer`, which includes:\n",
        "1.  Masked Self-Attention (to look at previously generated target tokens).\n",
        "2.  Encoder-Decoder Cross-Attention (to look at the input sentence's representation from the Encoder).\n",
        "3.  A Feed-Forward Network.\n",
        "\n",
        "Each `DecoderLayer` takes the output from the previous layer (or the initial target embeddings for the first layer) and the Encoder's output, and refines the representation of the target sequence. By stacking `N` of these layers (e.g., N=6 was used in the original \"Attention Is All You Need\" paper), the Decoder can learn to generate more coherent and contextually appropriate translations.\n",
        "\n",
        "The `Decoder` module itself will be structurally similar to our `Encoder` module:\n",
        "* It will contain a list of `DecoderLayer` instances.\n",
        "* Its `__call__` method will sequentially pass the data through these layers.\n",
        "* It will also likely include a final `RMSNorm` for stability.\n",
        "\n",
        "The output of this final `Decoder` stack will be a sequence of vectors, each representing a predicted token at that position in the target sentence. These vectors are almost ready to be turned into actual word predictions!"
      ],
      "metadata": {
        "id": "z6wgpw7srwWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transformer Building Block: Full Decoder ---\n",
        "\n",
        "class Decoder(nnx.Module):\n",
        "    \"\"\"\n",
        "    The full Decoder stack, composed of multiple DecoderLayers.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_layers: int,\n",
        "                 in_features: int,\n",
        "                 num_heads: int,\n",
        "                 hidden_dim: int,\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 activation_fn: Callable = jax.nn.relu,\n",
        "                 *,\n",
        "                 rngs: nnx.Rngs\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.in_features = in_features\n",
        "\n",
        "        self.layers = [\n",
        "            DecoderLayer(\n",
        "                in_features=in_features,\n",
        "                num_heads=num_heads,\n",
        "                hidden_dim=hidden_dim,\n",
        "                dropout_rate=dropout_rate,\n",
        "                activation_fn=activation_fn,\n",
        "                rngs=rngs\n",
        "            ) for _ in range(num_layers)\n",
        "        ]\n",
        "        self.final_norm = nnx.RMSNorm(num_features=in_features, rngs=rngs)\n",
        "\n",
        "    def __call__(self,\n",
        "                 x: jax.Array, # Target sequence embeddings\n",
        "                 encoder_output: jax.Array,\n",
        "                 causal_mask: Optional[jax.Array], # For target self-attention\n",
        "                 encoder_attention_mask: Optional[jax.Array], # For source padding in cross-attention\n",
        "                ) -> jax.Array:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input to the decoder. Shape: (batch_size, target_seq_len, in_features)\n",
        "            encoder_output: Output from the encoder. Shape: (batch_size, source_seq_len, in_features)\n",
        "            causal_mask: Boolean mask for masked self-attention on the target sequence.\n",
        "            encoder_attention_mask: Boolean mask for encoder-decoder attention (source padding).\n",
        "        Returns:\n",
        "            Output of the decoder. Shape: (batch_size, target_seq_len, in_features)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(\n",
        "                x,\n",
        "                encoder_output=encoder_output,\n",
        "                causal_mask=causal_mask,\n",
        "                encoder_attention_mask=encoder_attention_mask,\n",
        "            )\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HcVHMEzjrS_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Block 8: The Grand Assembly - Our Seq2Seq Transformer!\n",
        "\n",
        "The moment of truth approaches! We've painstakingly assembled all the crucial components:\n",
        "* `PositionalEmbedding`: To get our input tokens represented as vectors with positional awareness.\n",
        "* `Encoder`: A stack of `EncoderLayer`s to deeply understand the source sentence.\n",
        "* `Decoder`: A stack of `DecoderLayer`s to generate the target sentence, paying attention to both its own previous outputs and the encoder's wisdom.\n",
        "\n",
        "Now, it's time to put them all together into the final **Transformer** model!\n",
        "\n",
        "Our top-level `Transformer` module will:\n",
        "1.  Hold instances of the `PositionalEmbedding` layer (we might need one for the source language and one for the target, or a shared one if vocabularies are merged).\n",
        "2.  Hold an instance of our `Encoder`.\n",
        "3.  Hold an instance of our `Decoder`.\n",
        "4.  Include a final **Linear layer**. This layer takes the output of the Decoder (which is a sequence of vectors of size `in_features`) and projects each vector into a vector of size `target_vocab_size`. These are the **logits** – raw scores for each word in the target vocabulary. Applying a softmax function to these logits gives us probabilities for the next word.\n",
        "\n",
        "The `__call__` method of our `Transformer` will orchestrate the entire process:\n",
        "* Take source token IDs and target token IDs (for training/teacher forcing) as input.\n",
        "* Create the necessary attention masks.\n",
        "* Pass source tokens through the source embedding and then the Encoder.\n",
        "* Pass target tokens (usually shifted, e.g., `target[:, :-1]`) through the target embedding and then the Decoder, along with the Encoder's output and the masks.\n",
        "* Pass the Decoder's output through the final linear layer to get logits.\n",
        "\n",
        "This is it – the complete architecture! Let's build our masterpiece."
      ],
      "metadata": {
        "id": "COBiJEWmtLEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transformer Building Block: Full Seq2Seq Transformer Model ---\n",
        "\n",
        "# Masking helper functions (simplified versions for this tutorial context)\n",
        "# In a real application, you might use more robust versions or those from nnx.attention if available.\n",
        "def create_padding_mask(token_ids: jax.Array, pad_id: int = PAD_ID) -> jax.Array:\n",
        "    \"\"\"Creates a padding mask from token IDs.\n",
        "    Args:\n",
        "        token_ids: (batch_size, seq_len)\n",
        "        pad_id: The ID used for padding.\n",
        "    Returns:\n",
        "        A boolean mask of shape (batch_size, 1, 1, seq_len). True where not padded.\n",
        "    \"\"\"\n",
        "    # Mask is True where token_id is NOT the pad_id\n",
        "    mask = (token_ids != pad_id)\n",
        "    # Add dimensions for num_heads and query_length broadcasting for MHA\n",
        "    return mask[:, jnp.newaxis, jnp.newaxis, :]\n",
        "\n",
        "\n",
        "def create_causal_mask(target_token_ids: jax.Array, pad_id: int = PAD_ID) -> jax.Array:\n",
        "    \"\"\"Creates a causal (look-ahead) mask combined with a padding mask for the target sequence.\n",
        "    Args:\n",
        "        target_token_ids: (batch_size, target_seq_len)\n",
        "        pad_id: The ID used for padding.\n",
        "    Returns:\n",
        "        A boolean mask of shape (batch_size, 1, target_seq_len, target_seq_len).\n",
        "    \"\"\"\n",
        "    seq_len = target_token_ids.shape[1]\n",
        "    # 1. Create padding mask for the target sequence (B, 1, 1, TQL)\n",
        "    # `target_key_padding_mask` ensures we don't attend to padded keys in the target.\n",
        "    target_key_padding_mask = create_padding_mask(target_token_ids, pad_id)\n",
        "\n",
        "    # 2. Create causal mask (1, 1, TQL, TQL)\n",
        "    # True if key_pos <= query_pos\n",
        "    i = jnp.arange(seq_len)[:, jnp.newaxis]\n",
        "    j = jnp.arange(seq_len)[jnp.newaxis, :]\n",
        "    causal_part = (j <= i) # Lower triangular matrix\n",
        "    causal_mask_no_batch_heads = causal_part[jnp.newaxis, jnp.newaxis, :, :] # (1,1,TQL,TQL)\n",
        "\n",
        "    # 3. Broadcast causal_mask_no_batch_heads (1,1,TQL,TQL) with target_key_padding_mask (B,1,1,TQL)\n",
        "    combined_mask = jnp.logical_and(causal_mask_no_batch_heads, target_key_padding_mask)\n",
        "    return combined_mask.astype(jnp.bool_)\n",
        "\n",
        "\n",
        "class Transformer(nnx.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 embed_dim: int,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 num_heads: int,\n",
        "                 hidden_dim: int, # FeedForward hidden dim\n",
        "                 max_seq_len_pos_emb: int, # Max len for positional embedding table\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 activation_fn: Callable = jax.nn.relu,\n",
        "                 pad_id: int = PAD_ID,\n",
        "                 *,\n",
        "                 rngs: nnx.Rngs\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "        self.embedding = PositionalEmbedding(\n",
        "            max_sequence_length=max_seq_len_pos_emb,\n",
        "            vocab_size=vocab_size,\n",
        "            embed_dim=embed_dim,\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "        if dropout_rate > 0: # Main dropout after embeddings\n",
        "            self.emb_dropout = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
        "        else:\n",
        "            self.emb_dropout = None\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            num_layers=num_encoder_layers,\n",
        "            in_features=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            hidden_dim=hidden_dim,\n",
        "            dropout_rate=dropout_rate,\n",
        "            activation_fn=activation_fn,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            num_layers=num_decoder_layers,\n",
        "            in_features=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            hidden_dim=hidden_dim,\n",
        "            dropout_rate=dropout_rate,\n",
        "            activation_fn=activation_fn,\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "        self.final_linear = nnx.Linear(in_features=embed_dim, out_features=vocab_size, rngs=rngs)\n",
        "\n",
        "    def __call__(self,\n",
        "                 source_token_ids: jax.Array, # (batch_size, source_seq_len)\n",
        "                 target_token_ids: jax.Array, # (batch_size, target_seq_len) for teacher forcing\n",
        "                ) -> jax.Array: # Returns logits\n",
        "\n",
        "        # 1. Create Masks\n",
        "        # Source padding mask (for encoder self-attention & decoder cross-attention)\n",
        "        # Shape: (batch, 1, 1, source_seq_len)\n",
        "        source_padding_mask = create_padding_mask(source_token_ids, self.pad_id)\n",
        "\n",
        "        # Target causal mask (for decoder self-attention)\n",
        "        # Combines look-ahead prevention and target padding.\n",
        "        # Shape: (batch, 1, target_seq_len, target_seq_len)\n",
        "        # We'll use the target_token_ids that are fed for teacher forcing (e.g., target_ids[:, :-1])\n",
        "        # The mask should match the query/key length of the decoder's self-attention.\n",
        "        # If decoder input is target_ids[:, :-1], its length is target_seq_len - 1.\n",
        "        decoder_self_attn_input_ids = target_token_ids[:, :-1]\n",
        "        target_causal_mask = create_causal_mask(decoder_self_attn_input_ids, self.pad_id)\n",
        "\n",
        "        # 2. Source Embeddings & Encoder\n",
        "        source_embedded = self.embedding(source_token_ids)\n",
        "        if self.emb_dropout is not None:\n",
        "            source_embedded = self.emb_dropout(source_embedded)\n",
        "        encoder_output = self.encoder(source_embedded, attention_mask=source_padding_mask)\n",
        "\n",
        "        # 3. Target Embeddings (for decoder input - shifted right)\n",
        "        # Typically, for teacher forcing, decoder input is target_token_ids[:, :-1]\n",
        "        target_embedded_input = self.embedding(decoder_self_attn_input_ids)\n",
        "        if self.emb_dropout is not None:\n",
        "            target_embedded_input = self.emb_dropout(target_embedded_input)\n",
        "\n",
        "        # 4. Decoder\n",
        "        decoder_output = self.decoder(\n",
        "            target_embedded_input,\n",
        "            encoder_output=encoder_output,\n",
        "            causal_mask=target_causal_mask, # For decoder's self-attention\n",
        "            encoder_attention_mask=source_padding_mask, # For decoder's cross-attention\n",
        "        )\n",
        "\n",
        "        # 5. Final Linear layer to get logits\n",
        "        logits = self.final_linear(decoder_output)\n",
        "        # logits shape: (batch_size, target_seq_len-1, target_vocab_size)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "2YCoL8nutFJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Schooling Our Creation - Training Time! Optax & Orbax to the Rescue\n",
        "\n",
        "Bravo! Give yourself a round of applause, a pat on the back, or maybe just a satisfied sip of your beverage. We've successfully designed and (notionally) built all the intricate pieces of our Transformer model. It's standing there, a magnificent assembly of embeddings, attention mechanisms, feed-forward networks, encoders, and decoders, all powered by Flax NNX.\n",
        "\n",
        "But right now, our Transformer is like a brilliant student who hasn't been to school yet. It has all the potential, but its internal weights and biases are just random numbers. It doesn't know English from Spanish from a recipe for banana bread.\n",
        "\n",
        "It's time for **training**! This is where we show our model vast amounts of English-Spanish sentence pairs and let it learn to translate. We'll use:\n",
        "\n",
        "* **A Loss Function:** To measure how \"wrong\" our model's predictions are compared to the actual Spanish translations. (Cross-entropy is the go-to for this).\n",
        "* **An Optimizer (`optax`):** A clever algorithm (like AdamW) that takes the \"wrongness\" signal from the loss function and nudges the model's parameters in the right direction to make it less wrong next time. `optax` is JAX's premier optimization library.\n",
        "* **Our Grain Data Pipeline:** To efficiently feed batches of training data to the model.\n",
        "* **A Training Loop:** The main script that orchestrates epochs, batches, forward passes, loss calculation, backward passes (gradient computation), and optimizer steps.\n",
        "* **Checkpointing (`orbax-checkpoint`):** To save our model's hard-earned knowledge periodically, because training can take a while, and we don't want to lose progress if our cat walks across the keyboard (or, you know, a real issue occurs).\n",
        "\n",
        "We'll also set up to train two variants: a \"small\" model and a \"big\" model, to see how size affects performance. This is where the `tqdm` progress bars will become our best friends.\n",
        "\n",
        "Let's get ready to send our Transformer to translation school!"
      ],
      "metadata": {
        "id": "A-ei-bJr1OhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Setup: Hyperparameters & Model Instantiation ---\n",
        "\n",
        "# We'll define two sets of hyperparameters: one for a \"small\" model (quick to train for a tutorial)\n",
        "# and one for a \"bigger\" model (closer to something that might give reasonable results).\n",
        "\n",
        "# Hyperparameters for a \"SMALL\" model (for quick tutorial training)\n",
        "config_small = {\n",
        "    \"embed_dim\": 128,  # d_model: Dimension of embeddings and model\n",
        "    \"num_encoder_layers\": 2,\n",
        "    \"num_decoder_layers\": 2,\n",
        "    \"num_heads\": 4,\n",
        "    \"hidden_dim\": 256, # FeedForward hidden dimension (d_ff)\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"activation_fn\": jax.nn.relu, # Or jax.nn.gelu\n",
        "    \"label_smoothing\": 0.1, # For loss function\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"warmup_steps\": 1000, # For learning rate schedule\n",
        "    \"batch_size\": BATCH_SIZE, # From Grain setup (Cell 11/12, e.g., 64)\n",
        "    \"num_epochs\": 3, # Keep low for tutorial\n",
        "}\n",
        "\n",
        "# Hyperparameters for a \"BIGGER\" model (more capable, takes longer)\n",
        "config_big = {\n",
        "    \"embed_dim\": 256, # d_model\n",
        "    \"num_encoder_layers\": 4, # Typically 6 in \"base\" Transformer\n",
        "    \"num_decoder_layers\": 4, # Typically 6 in \"base\" Transformer\n",
        "    \"num_heads\": 8,\n",
        "    \"hidden_dim\": 1024, # d_ff (usually 4 * d_model)\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"activation_fn\": jax.nn.gelu, # GeLU is common in larger models\n",
        "    \"label_smoothing\": 0.1,\n",
        "    \"learning_rate\": 5e-4, # Often smaller for bigger models\n",
        "    \"warmup_steps\": 2000,\n",
        "    \"batch_size\": BATCH_SIZE, # Adjust if GPU memory is an issue\n",
        "    \"num_epochs\": 5, # Still low, but more than small\n",
        "}\n",
        "\n",
        "current_config = config_big\n",
        "\n",
        "print(\"--- Model Configuration ---\")\n",
        "for key_conf, value_conf in current_config.items():\n",
        "    print(f\"{key_conf}: {value_conf}\")\n",
        "\n",
        "# Instantiate the Transformer model using the chosen config\n",
        "transformer_model = Transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embed_dim=current_config[\"embed_dim\"],\n",
        "    num_encoder_layers=current_config[\"num_encoder_layers\"],\n",
        "    num_decoder_layers=current_config[\"num_decoder_layers\"],\n",
        "    num_heads=current_config[\"num_heads\"],\n",
        "    hidden_dim=current_config[\"hidden_dim\"],\n",
        "    max_seq_len_pos_emb=MAX_SEQ_LEN,\n",
        "    dropout_rate=current_config[\"dropout_rate\"],\n",
        "    activation_fn=current_config[\"activation_fn\"],\n",
        "    pad_id=PAD_ID,\n",
        "    rngs=nnx.Rngs(0)\n",
        ")\n",
        "print(f\"\\nTransformer model instantiated with '{'small' if current_config==config_small else 'big'}' config.\")\n",
        "print(f\"  Model embed_dim: {current_config['embed_dim']}\")\n",
        "print(f\"  Max positional embedding length: {MAX_SEQ_LEN}\")\n",
        "\n",
        "nnx.display(transformer_model)"
      ],
      "metadata": {
        "id": "B6ajPav1z6N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = current_config.get(\"learning_rate\", 1e-3)\n",
        "warmup_steps = current_config.get(\"warmup_steps\", 1000)\n",
        "\n",
        "# Create a learning rate schedule\n",
        "lr_schedule = optax.warmup_exponential_decay_schedule(\n",
        "    init_value=0.0, # Start from 0\n",
        "    peak_value=learning_rate, # Peak at the specified learning rate\n",
        "    warmup_steps=warmup_steps,\n",
        "    transition_steps=current_config.get(\"num_epochs\", 5) * (NUM_EXAMPLES_CAP // BATCH_SIZE) // 2, # Example: decay over half the remaining steps\n",
        "    decay_rate=0.8, # Example decay rate\n",
        "    end_value=learning_rate / 10, # Example end value\n",
        ")\n",
        "\n",
        "optimizer = nnx.Optimizer(\n",
        "    transformer_model,\n",
        "    optax.adamw(learning_rate=lr_schedule, weight_decay=1e-4),\n",
        ")"
      ],
      "metadata": {
        "id": "D9h5JLxXEMav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(\n",
        "    logits: jax.Array, # (batch_size, target_seq_len-1, target_vocab_size)\n",
        "    targets: jax.Array, # (batch_size, target_seq_len-1) of integer token IDs\n",
        "    target_vocab_size: int,\n",
        "    pad_id: int = PAD_ID # Use the global PAD_ID\n",
        ") -> jax.Array: # Returns scalar loss\n",
        "    \"\"\"Calculates cross-entropy loss, ignoring padding.\"\"\"\n",
        "\n",
        "    # 1. Convert targets to one-hot encoding.\n",
        "    # targets_one_hot shape: (batch_size, target_seq_len-1, target_vocab_size)\n",
        "    targets_one_hot = jax.nn.one_hot(targets, num_classes=target_vocab_size)\n",
        "\n",
        "    # 2. Calculate softmax cross-entropy loss using Optax's numerically stable function.\n",
        "    # loss shape: (batch_size, target_seq_len-1)\n",
        "    loss = optax.softmax_cross_entropy(logits=logits, labels=targets_one_hot)\n",
        "\n",
        "    # 3. Create a mask to ignore padding tokens in the loss calculation.\n",
        "    # padding_mask shape: (batch_size, target_seq_len-1)\n",
        "    # True where target token is NOT pad_id.\n",
        "    padding_mask = (targets != pad_id)\n",
        "\n",
        "    # 4. Apply the padding mask to the loss.\n",
        "    loss = loss * padding_mask\n",
        "\n",
        "    # 5. Calculate the mean loss over non-padded tokens.\n",
        "    # Sum loss over sequence and batch.\n",
        "    masked_loss_sum = jnp.sum(loss)\n",
        "\n",
        "    # Number of non-padded tokens.\n",
        "    num_non_padding_tokens = jnp.sum(padding_mask)\n",
        "\n",
        "    # Calculate mean loss, adding epsilon to denominator to avoid division by zero.\n",
        "    mean_loss = masked_loss_sum / (num_non_padding_tokens + 1e-8)\n",
        "\n",
        "    return mean_loss"
      ],
      "metadata": {
        "id": "OM7fWXzd3NLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Main Event: Training Loop Shenanigans! (Ready, Set, TRAIN! ☕)\n",
        "\n",
        "We've reached a pivotal moment! Our magnificent Transformer, meticulously assembled with Flax NNX, stands ready. Our data, curated by the Grain pipeline, is eager to be consumed. Our optimizer, powered by Optax, is itching to sculpt the model's parameters. And our loss function is poised to pass judgment.\n",
        "\n",
        "It's time to bring all these elements together in the **training loop**. This is where the iterative process of learning happens:\n",
        "\n",
        "1.  **Epoch by Epoch:** We'll go through our entire training dataset multiple times (each pass is an \"epoch\").\n",
        "2.  **Batch by Batch:** In each epoch, our Grain `DataLoader` will serve up tasty mini-batches of English-Spanish sentence pairs.\n",
        "3.  **The `train_step`:** For each batch, a special JIT-compiled function will:\n",
        "    * Feed the source and target sentences to our Transformer model (this is the \"forward pass\").\n",
        "    * Calculate how \"off\" the model's predictions are using our `calculate_loss` function.\n",
        "    * Compute the gradients – figuring out how each model parameter contributed to that \"off-ness.\"\n",
        "    * Ask our Optax optimizer to update the model parameters based on these gradients, nudging them towards making better predictions.\n",
        "4.  **Evaluation (Periodically):** It's good practice to occasionally check how our model is doing on a separate validation set (we'll simplify this for now and just focus on the training step, but in a full setup, you'd have an `eval_step`).\n",
        "5.  **Progress Tracking:** We'll use `tqdm` to give us those satisfying progress bars, so we know our computer isn't just idly dreaming of electric sheep.\n",
        "6.  **Saving Our Brains (`orbax-checkpoint`):** We'll set up checkpointing to save the model's state (its learned parameters and the optimizer's state) so we can resume training or use the trained model later without starting from scratch.\n",
        "\n",
        "This loop is the heart of machine learning. It's where the \"learning\" in \"deep learning\" actually occurs. It might take a while, so this is a good time to ensure your coffee (or tea, or beverage of choice) is topped up!"
      ],
      "metadata": {
        "id": "fN-MEGh494IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Setup: The `train_step` Function ---\n",
        "\n",
        "# This function will perform one step of training:\n",
        "# - Forward pass through the model\n",
        "# - Loss calculation\n",
        "# - Gradient computation\n",
        "# - Parameter update\n",
        "\n",
        "# The loss function needs access to the model, data, and some configs.\n",
        "# We'll define a helper that `nnx.value_and_grad` can differentiate.\n",
        "def loss_fn_for_grad(\n",
        "    model: Transformer, # The Transformer model instance\n",
        "    batch: Dict[str, jax.Array],\n",
        "    target_vocab_size: int,\n",
        "    pad_id: int,\n",
        "    ) -> jax.Array:\n",
        "    \"\"\"\n",
        "    Calculates loss for a given model and batch.\n",
        "    This function is what nnx.value_and_grad will differentiate.\n",
        "    The first argument `model` is what gradients will be computed against.\n",
        "    \"\"\"\n",
        "    source_token_ids = batch['encoder_input_tokens']\n",
        "    target_token_ids_full = batch['decoder_input_tokens'] # Shape (B, T) includes BOS and EOS\n",
        "\n",
        "    # For teacher forcing:\n",
        "    # Decoder input is target_token_ids_full[:, :-1] (BOS, tok1, ..., tokN)\n",
        "    # Actual targets for loss are target_token_ids_full[:, 1:] (tok1, ..., tokN, EOS)\n",
        "    # The model's __call__ method handles taking target_token_ids_full[:, :-1] internally for its decoder input.\n",
        "    # So, we pass the full target_token_ids_full to the model.\n",
        "    # The model's __call__ will output logits for predicting target_token_ids_full[:, 1:].\n",
        "\n",
        "    logits = model(\n",
        "        source_token_ids=source_token_ids,\n",
        "        target_token_ids=target_token_ids_full, # Pass the full target sequence\n",
        "    )\n",
        "    # Logits shape: (batch_size, target_seq_len-1, target_vocab_size)\n",
        "\n",
        "    # Actual targets for the loss are shifted\n",
        "    actual_targets = target_token_ids_full[:, 1:] # Shape (B, T-1)\n",
        "\n",
        "    # Ensure logits and actual_targets sequence lengths match\n",
        "    if logits.shape[1] != actual_targets.shape[1]:\n",
        "        raise ValueError(\n",
        "            f\"Logits sequence length ({logits.shape[1]}) and target sequence length ({actual_targets.shape[1]}) \"\n",
        "            \"mismatch after model call and target shifting.\"\n",
        "        )\n",
        "\n",
        "    loss = calculate_loss( # Using the simplified version from (revised) Cell 34\n",
        "        logits=logits,\n",
        "        targets=actual_targets,\n",
        "        target_vocab_size=target_vocab_size,\n",
        "        pad_id=pad_id\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "# Create the gradient function using nnx.value_and_grad\n",
        "# It will compute the loss and gradients w.r.t. the first argument of loss_fn_for_grad (the model).\n",
        "# The gradients returned will be a PyTree with the same structure as `nnx.Param`s in the model.\n",
        "compute_loss_and_grads = nnx.value_and_grad(loss_fn_for_grad, argnums=0)\n",
        "\n",
        "def train_step(\n",
        "    model: Transformer,\n",
        "    batch: Dict[str, jax.Array],\n",
        "    optimizer: nnx.Optimizer,\n",
        "    target_vocab_size: int, # Passed as static arg or from config\n",
        "    pad_id: int             # Passed as static arg or from config\n",
        "    ) -> jax.Array:\n",
        "    \"\"\"Performs a single training step.\"\"\"\n",
        "\n",
        "    # Compute loss and gradients.\n",
        "    # The model's internal 'dropout' RngStream will be used and updated.\n",
        "    loss_value, grads = compute_loss_and_grads(\n",
        "        model, # Pass the model instance\n",
        "        batch,\n",
        "        target_vocab_size,\n",
        "        pad_id,\n",
        "    )\n",
        "\n",
        "    # Optimizer update\n",
        "    optimizer.update(grads)\n",
        "\n",
        "    return loss_value\n",
        "\n",
        "def eval_step(\n",
        "    model: Transformer,\n",
        "    batch: Dict[str, jax.Array],\n",
        "    target_vocab_size: int, # Passed as static arg or from config\n",
        "    pad_id: int             # Passed as static arg or from config\n",
        "    ) -> jax.Array:\n",
        "    \"\"\"Performs a single evaluation step.\"\"\"\n",
        "\n",
        "    source_token_ids = batch['encoder_input_tokens']\n",
        "    target_token_ids_full = batch['decoder_input_tokens']\n",
        "\n",
        "    logits = model(\n",
        "        source_token_ids=source_token_ids,\n",
        "        target_token_ids=target_token_ids_full,\n",
        "    )\n",
        "\n",
        "    actual_targets = target_token_ids_full[:, 1:]\n",
        "\n",
        "    if logits.shape[1] != actual_targets.shape[1]:\n",
        "        raise ValueError(\"Logits and target sequence length mismatch in eval_step.\")\n",
        "\n",
        "    loss = calculate_loss(\n",
        "        logits=logits,\n",
        "        targets=actual_targets,\n",
        "        target_vocab_size=target_vocab_size,\n",
        "        pad_id=pad_id\n",
        "    )\n",
        "    return loss"
      ],
      "metadata": {
        "id": "6fytAQSM9k2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_size_str = \"small\" if current_config == config_small else \"big\""
      ],
      "metadata": {
        "id": "enxi7wB8E1bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_url = f\"https://raw.githubusercontent.com/mridul-sahu/transformers-from-scratch/main/transformer_eng_spa_{model_size_str}.zip\"\n",
        "checkpoints_path = pathlib.Path(\"/tmp/checkpoints/\")\n",
        "checkpoints_path.mkdir(parents=True, exist_ok=True)\n",
        "checkpoint_zip_path = checkpoints_path / f\"transformer_eng_spa_{model_size_str}.zip\"\n",
        "checkpoint_dir = checkpoints_path / f\"transformer_eng_spa_{model_size_str}\"\n",
        "\n",
        "# --- Download and Extract Dataset ---\n",
        "if not checkpoint_dir.exists():\n",
        "    if not checkpoint_zip_path.exists():\n",
        "        print(f\"Downloading checkpoints from {checkpoint_url}...\")\n",
        "        urllib.request.urlretrieve(checkpoint_url, checkpoint_zip_path)\n",
        "        print(f\"Checkpoints downloaded to {checkpoint_zip_path}\")\n",
        "    else:\n",
        "        print(f\"Checkpoints zip file already exists at {checkpoint_zip_path}\")\n",
        "\n",
        "    print(f\"Extracting {checkpoint_zip_path}...\")\n",
        "    with zipfile.ZipFile(checkpoint_zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(checkpoints_path)\n",
        "    print(f\"Checkpoints extracted to {checkpoint_dir}\")\n",
        "else:\n",
        "    print(f\"Checkpoints directory {checkpoint_dir} already exists. Skipping download and extraction.\")\n",
        "\n",
        "print(f\"Checkpoints will be saved in: {checkpoint_dir}\")"
      ],
      "metadata": {
        "id": "RliT_C2HEggC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_options = ocp.CheckpointManagerOptions(\n",
        "    save_decision_policy=ocp.checkpoint_managers.ContinuousCheckpointingPolicy(\n",
        "        minimum_interval_secs=5\n",
        "    ),\n",
        "    max_to_keep=1,\n",
        ")\n",
        "\n",
        "checkpoint_manager = ocp.CheckpointManager(checkpoint_dir, options=checkpoint_options)\n",
        "\n",
        "def checkpoint(step, model, dataset_iterator, force=False) -> bool:\n",
        "  return checkpoint_manager.save(step, args=ocp.args.Composite(\n",
        "      model=ocp.args.StandardSave(nnx.state(model, nnx.Param)),\n",
        "      dataset_iterator=pygrain.PyGrainCheckpointSave(dataset_iterator)), force=force)"
      ],
      "metadata": {
        "id": "rD8SZ2rpCbWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Training Gauntlet! Let the Epochs Roll!\n",
        "\n",
        "We're standing at the precipice of creation (or at least, of a somewhat functional translator). All our carefully crafted components are ready. Now, we need to put our `Transformer` model through its paces, forcing it to learn from the data batch by painstaking batch.\n",
        "\n",
        "This is the **main training loop**. It's where:\n",
        "* We iterate through our dataset for a set number of \"epochs\" (an epoch is one full pass through the data).\n",
        "* Our `Grain` data loader serves up mini-batches of English-Spanish pairs.\n",
        "* For each batch, our JIT-compiled `train_step` function does the heavy lifting:\n",
        "    * Makes predictions (forward pass).\n",
        "    * Calculates how wrong those predictions are (loss).\n",
        "    * Figures out how to make them less wrong (gradients).\n",
        "    * Updates the model's parameters (optimizer step).\n",
        "* We'll use `tqdm` to create those delightful progress bars, giving us a sense of progress and a chance to ponder the mysteries of the universe while the GPU fans whir.\n",
        "* Crucially, our `Orbax` checkpoint manager will be standing by to save our model's evolving intelligence (and the optimizer's state) at regular intervals. This means if your laptop decides to take an unscheduled nap, we won't lose all our hard-earned training!\n",
        "\n",
        "We'll also keep an eye on the loss to see if our model is actually learning something or just developing a taste for random token generation. Lower loss, we hope, means a smarter model!\n",
        "\n",
        "So, take a deep breath, cross your fingers (it's a valid ML technique, trust me), and let's start the training!"
      ],
      "metadata": {
        "id": "V9I9vcXYDf3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_step_jitted = nnx.jit(train_step, donate_argnames=(\"optimizer\",), static_argnames=(\"target_vocab_size\", \"pad_id\"))\n",
        "eval_step_jitted = nnx.jit(eval_step, donate_argnames=(\"model\",), static_argnames=(\"target_vocab_size\", \"pad_id\"))\n",
        "\n",
        "num_epochs = current_config.get(\"num_epochs\", 3)\n",
        "train_data_loader, eval_data_loader = get_dataset_iterators(num_epochs=num_epochs)\n",
        "train_data_iterator = iter(train_data_loader)\n",
        "eval_data_iterator = iter(eval_data_loader)"
      ],
      "metadata": {
        "id": "rh2D4-Yyzcvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model in train mode\n",
        "transformer_model.train()\n",
        "def train_loop():\n",
        "  print(\"--- Starting Training Loop ---\")\n",
        "  train_losses = []\n",
        "  latest_step = checkpoint_manager.latest_step()\n",
        "  if latest_step is not None:\n",
        "    ckpt = checkpoint_manager.restore(\n",
        "        None,\n",
        "        args=ocp.args.Composite(\n",
        "            model=ocp.args.StandardRestore(nnx.state(transformer_model, nnx.Param)),\n",
        "            dataset_iterator=pygrain.PyGrainCheckpointRestore(train_data_iterator)\n",
        "      ),\n",
        "    )\n",
        "    nnx.update(transformer_model, ckpt.model)\n",
        "    print(f\"Restored from {latest_step=}\")\n",
        "  total_loss = 0\n",
        "  last_checkpoint = 0\n",
        "  total_batches = num_epochs * min(NUM_EXAMPLES_CAP, len(all_sentence_pairs)) // BATCH_SIZE\n",
        "  current_step = latest_step or 0\n",
        "  with tqdm(desc=f\"Training Step\", initial=current_step, total=total_batches) as pbar_batch:\n",
        "    for batch in train_data_iterator:\n",
        "      current_step += 1\n",
        "      # Perform one training step\n",
        "      loss_value = train_step_jitted(\n",
        "          transformer_model,\n",
        "          batch,\n",
        "          optimizer,\n",
        "          VOCAB_SIZE,\n",
        "          PAD_ID,\n",
        "      )\n",
        "      total_loss += loss_value\n",
        "      train_losses.append(loss_value)\n",
        "      pbar_batch.update(1)\n",
        "      if current_step % 20 == 0:\n",
        "        pbar_batch.set_postfix(loss=f\"{loss_value:.4f}\", avg_loss=f\"{total_loss/(current_step+1):.4f}\", last_checkpoint=f\"{last_checkpoint}\")\n",
        "      # Checkpoint the model and iterator state\n",
        "      if checkpoint(current_step, transformer_model, train_data_iterator):\n",
        "        print(f\"Checkpointing at step {current_step}\")\n",
        "        last_checkpoint = current_step\n",
        "  latest_step = checkpoint_manager.latest_step()\n",
        "  if latest_step is None or latest_step < current_step:\n",
        "    if checkpoint(current_step, transformer_model, train_data_iterator, force=True):\n",
        "      print(f\"Checkpointing at step {current_step}\")\n",
        "  checkpoint_manager.wait_until_finished()\n",
        "  print(f\"Completed training total steps: {current_step}\")\n",
        "\n",
        "\n",
        "  return train_losses"
      ],
      "metadata": {
        "id": "Kp18ICwlDT5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = train_loop()"
      ],
      "metadata": {
        "id": "YzTGGIjIPbCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-Training Ponderings: Genius or Just Good at Guessing?\n",
        "\n",
        "Phew! The training loop has run its course. Our GPU fans can take a breather, and so can we. We've thrown data at our Transformer, tweaked its virtual brain cells with `optax`, and diligently saved its progress with `Orbax`.\n",
        "\n",
        "But the big question looms: **Did it actually learn to translate?** Or has it just gotten very good at predicting common Spanish words without understanding the English input?\n",
        "\n",
        "* The **training loss** gives us a hint. If it went down significantly, that's a good sign! It means the model got better at predicting the target sequences it was shown.\n",
        "* However, low training loss doesn't always guarantee good generalization to new, unseen sentences. The model might have \"memorized\" the training data a bit too well (this is called overfitting).\n",
        "\n",
        "To truly gauge its abilities, we'd typically:\n",
        "1.  **Evaluate on a held-out validation set:** Calculate loss and other metrics (like perplexity or accuracy if applicable) on data the model *didn't* see during training. This gives a more honest assessment.\n",
        "2.  **Qualitative Check:** Actually look at some translations it produces for new sentences! Does it make sense? Is it grammatically correct? Does it capture the nuance?\n",
        "\n",
        "For this tutorial, our next step will be to plot the training loss to visualize its learning journey. Then, we'll gear up to try some live translations using our sampler!"
      ],
      "metadata": {
        "id": "AG2tuyEsPfz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualizing Learning: Plotting Training Loss ---\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_losses(losses, chunk_size = 100):\n",
        "  steps_ran = len(losses)\n",
        "  # Calculate the mean of every chunk_size steps\n",
        "  chunk_size = 100\n",
        "  #The number of chunks we can make\n",
        "  num_chunks = steps_ran // chunk_size\n",
        "\n",
        "  mean_losses = []\n",
        "  #The x-values for our plot\n",
        "  mean_steps = []\n",
        "\n",
        "  for i in range(num_chunks):\n",
        "      start_index = i * chunk_size\n",
        "      end_index = start_index + chunk_size\n",
        "      chunk = losses[start_index:end_index]\n",
        "      mean_losses.append(np.mean(chunk))\n",
        "      mean_steps.append(start_index + chunk_size) # Plotting at the end of the chunk\n",
        "\n",
        "  # Handle the remaining steps if steps_ran is not a perfect multiple of chunk_size\n",
        "  if steps_ran % chunk_size != 0:\n",
        "      remaining_chunk = losses[num_chunks * chunk_size:]\n",
        "      if len(remaining_chunk) > 0: #Make sure there actually are remaining steps\n",
        "          mean_losses.append(np.mean(remaining_chunk))\n",
        "          mean_steps.append(steps_ran)\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(mean_steps, mean_losses, marker='o', linestyle='-', color='dodgerblue')\n",
        "  plt.title(f'Mean Training Loss (Every {chunk_size} Steps) Over {steps_ran} Steps (Model: {model_size_str.capitalize()})')\n",
        "  plt.xlabel(f'Steps (Aggregated every {chunk_size})')\n",
        "  plt.ylabel('Mean Training Loss')\n",
        "  # Adjust x-ticks to match the mean_steps\n",
        "  plt.xticks(mean_steps, rotation=45) # Rotate for better readability if many ticks\n",
        "  plt.grid(True, linestyle='--', alpha=0.7)\n",
        "  plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "  plt.show()\n",
        "\n",
        "plot_losses(train_losses)\n",
        "\n",
        "print(\"Remember: This is training loss. A separate validation loss plot would be ideal\")\n",
        "print(\"to monitor for overfitting and for hyperparameter tuning (e.g., early stopping).\")"
      ],
      "metadata": {
        "id": "_DzKqS0eCepW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, our model has been trained (hopefully without too much drama), and we've even peeked at its training loss curve. Lower loss is nice, but for a translation task, seeing actual translated examples gives us a more direct feel for its performance.\n",
        "\n",
        "We have now built a `Sampler` to generate translations from our model. This will allow us to input English sentences and see the Spanish translations our Transformer produces. We'll be using a top-k sampling strategy for generation.\n",
        "\n",
        "To do this, we'll:\n",
        "1.  Take some English sentences from our evaluation dataset.\n",
        "2.  Use our trained `Transformer` model (in `eval` mode with the `Sampler`) to generate Spanish translations.\n",
        "3.  Look at these generated translations alongside the original English and the expected Spanish translations.\n",
        "\n",
        "Let's get our model ready to show us what it has learned!"
      ],
      "metadata": {
        "id": "lffEUgyeVrjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Our Sampler: Generating Translations Autoregressively\n",
        "\n",
        "Now that we've trained our Transformer, we need a mechanism to use it for actual translation. This is where the `Sampler` comes in. Unlike during training, where the decoder receives the correct \"ground truth\" tokens, during inference (translation), the decoder generates the output sequence one token at a time. This process is called **autoregressive decoding**.\n",
        "\n",
        "Here's how we'll build our `Sampler` class to handle this:\n",
        "\n",
        "1.  **Initialization:** The `Sampler` will be initialized with:\n",
        "    * Our trained `Transformer` model.\n",
        "    * The same `SentencePieceProcessor` (`tokenizer`) used during training.\n",
        "    * The maximum length of the output sequence (`max_output_len`).\n",
        "    * A function to determine the next token, defaulting to a top-k sampling strategy.\n",
        "\n",
        "2.  **Setting up the Model for Inference:**\n",
        "    * The model will be set to evaluation mode (`model.eval()`) to disable training-specific behavior like dropout.\n",
        "    * A crucial optimization for efficient autoregressive decoding is **KV caching**.  The `Sampler` will configure the decoder's multi-head attention layers to use KV caching. This means that the keys and values from previous decoder steps are stored and reused, avoiding redundant computations.\n",
        "\n",
        "3.  **The Generation Loop:** The core of the `Sampler` will be an iterative process:\n",
        "    * The encoder processes the input (English) sentence *once* to produce the encoder output.\n",
        "    * The decoder starts with a beginning-of-sequence (BOS) token.\n",
        "    * For each subsequent step:\n",
        "        * The decoder takes the previously generated token(s) as input.\n",
        "        * The model predicts the logits for the next token.\n",
        "        * A sampling function (like greedy decoding or top-k sampling) selects the next token ID.\n",
        "        * The selected token is appended to the generated sequence.\n",
        "        * The loop continues until an end-of-sequence (EOS) token is generated or the maximum length is reached.\n",
        "\n",
        "By encapsulating this logic, the `Sampler` provides a clean and reusable way to generate translations from our trained Transformer model."
      ],
      "metadata": {
        "id": "0b9xgeR_JdUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "def get_next_token_top_k(\n",
        "    logits: jax.Array,\n",
        "    key: jax.Array,\n",
        "    k: int = 5,\n",
        "    temperature: float = 1.0\n",
        ") -> jax.Array:\n",
        "    \"\"\"\n",
        "    Samples a token using Top-K filtering.\n",
        "    - logits: (batch_size, vocab_size)\n",
        "    - key: JAX PRNGKey\n",
        "    - k: Integer, the number of top logits to consider.\n",
        "    - temperature: Float for scaling logits before Top-K.\n",
        "    \"\"\"\n",
        "    batch_size, vocab_size = logits.shape\n",
        "\n",
        "    # Apply temperature\n",
        "    safe_temperature = jnp.maximum(temperature, 1e-8)\n",
        "    logits = logits / safe_temperature\n",
        "\n",
        "    # Get top_k values and their original indices\n",
        "    top_k_logits, top_k_indices = jax.lax.top_k(logits, k=min(k, vocab_size)) # Shape: (B, K) for both\n",
        "\n",
        "    # Create a mask to select only the top_k logits for categorical sampling\n",
        "    # We'll sample from the reduced set of top_k_logits directly.\n",
        "    # jax.random.categorical can directly sample from these top_k_logits.\n",
        "    # The output of categorical will be indices into the K dimension (0 to K-1).\n",
        "    sampled_indices_in_top_k = jax.random.categorical(key, top_k_logits, axis=-1) # Shape: (B,)\n",
        "\n",
        "    # Gather the original token IDs corresponding to the sampled indices\n",
        "    # For each item in the batch, pick the token from its top_k_indices\n",
        "    next_token_ids = jnp.squeeze(\n",
        "        jnp.take_along_axis(\n",
        "            top_k_indices, # Shape (B, K)\n",
        "            sampled_indices_in_top_k[:, jnp.newaxis], # Shape (B, 1) -> ensure correct broadcasting for gather\n",
        "            axis=-1\n",
        "        )\n",
        "    ) # Shape: (B,)\n",
        "    return next_token_ids\n",
        "\n",
        "class Sampler:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Transformer,                 # Your trained Transformer model\n",
        "        tokenizer: spm.SentencePieceProcessor,  # Your SentencePiece tokenizer\n",
        "        max_output_len: int,\n",
        "        gen_next_token_fn = get_next_token_top_k,\n",
        "    ):\n",
        "        self.model = nnx.clone(model)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_output_len = max_output_len\n",
        "        self.bos_id = self.tokenizer.bos_id()\n",
        "        self.eos_id = self.tokenizer.eos_id()\n",
        "        self.pad_id = self.tokenizer.pad_id()\n",
        "\n",
        "        sample_function = partial(Sampler.generate_step_for_batch, gen_next_token_fn=gen_next_token_fn)\n",
        "        self.jitted_sample_fn = nnx.jit(sample_function, static_argnames=('max_output_len','bos_id', 'eos_id', 'pad_id'))\n",
        "        self._setup_model(self.model)\n",
        "\n",
        "    def _setup_model(self, model):\n",
        "        # 1. Prepare the model for evaluation and decoding mode\n",
        "        model.eval()  # Sets training=False for all submodules (e.g., for dropout)\n",
        "\n",
        "        # 2. Configure MultiHeadAttention layers in the Decoder\n",
        "        for decoder_layer in model.decoder.layers:\n",
        "            # Enable KV caching for self-attention in decoder layers\n",
        "            if hasattr(decoder_layer, \"masked_self_attention\") and isinstance(\n",
        "                decoder_layer.masked_self_attention, nnx.MultiHeadAttention\n",
        "            ):\n",
        "                decoder_layer.masked_self_attention.decode = True\n",
        "\n",
        "    def _initialize_and_reset_caches(self, batch_size: int, model: Transformer):\n",
        "        \"\"\"\n",
        "        Initializes KV caches for decoder's self-attention layers\n",
        "        and resets their cache indices.\n",
        "        This should be called before starting generation for a new batch.\n",
        "        \"\"\"\n",
        "        # Define the input shape that MHAs will use to determine their cache sizes.\n",
        "        # This shape is (batch_size, max_sequence_length, features_of_input_to_MHA_projection_layers).\n",
        "        cache_init_shape = (\n",
        "            batch_size,\n",
        "            self.max_output_len,\n",
        "            model.embedding.embed_dim, # Features input to QKV projections in MHA\n",
        "        )\n",
        "        # Use a common dtype, e.g., float32 or derived from model parameters\n",
        "        dtype = jnp.float32\n",
        "\n",
        "        for decoder_layer in model.decoder.layers:\n",
        "            if (\n",
        "                hasattr(decoder_layer, \"masked_self_attention\")\n",
        "                and isinstance(decoder_layer.masked_self_attention, nnx.MultiHeadAttention)\n",
        "            ):\n",
        "                mha_self_attn = decoder_layer.masked_self_attention\n",
        "                mha_self_attn.init_cache(cache_init_shape, dtype=dtype)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_step_for_batch(\n",
        "        max_output_len: int,\n",
        "        bos_id: int,\n",
        "        eos_id: int,\n",
        "        pad_id: int,\n",
        "        model: Transformer,\n",
        "        source_token_ids: jax.Array,\n",
        "        loop_key: jax.Array,\n",
        "        gen_next_token_fn,\n",
        "    ) -> jax.Array:\n",
        "        \"\"\"\n",
        "        JIT-compiled core generation logic for a batch.\n",
        "        Assumes caches have been initialized and reset.\n",
        "        \"\"\"\n",
        "        batch_size = source_token_ids.shape[0]\n",
        "\n",
        "        # --- Encoder Pass (once per batch) ---\n",
        "        source_padding_mask = create_padding_mask(source_token_ids, pad_id)\n",
        "\n",
        "        source_embedded = model.embedding(source_token_ids)\n",
        "        if model.emb_dropout is not None:\n",
        "            source_embedded = model.emb_dropout(source_embedded)\n",
        "\n",
        "        encoder_output = model.encoder(\n",
        "            source_embedded, attention_mask=source_padding_mask\n",
        "        )\n",
        "\n",
        "        # --- Decoder Loop ---\n",
        "        # Initialize current_tokens with BOS ID for each sequence in the batch\n",
        "        current_tokens = jnp.full((batch_size, 1), bos_id, dtype=jnp.int32)\n",
        "\n",
        "        # Array to store all generated tokens, initialized with PAD ID\n",
        "        generated_tokens = jnp.full(\n",
        "            (batch_size, max_output_len), pad_id, dtype=jnp.int32\n",
        "        )\n",
        "        # Place BOS tokens at the beginning of each sequence\n",
        "        generated_tokens = generated_tokens.at[:, 0].set(current_tokens[:, 0])\n",
        "\n",
        "        # Boolean array to track which sequences have finished (generated EOS)\n",
        "        done_flags = jnp.zeros((batch_size,), dtype=jnp.bool_)\n",
        "\n",
        "        # Autoregressive decoding loop\n",
        "        for step in range(max_output_len - 1): # -1 because we predict one token ahead\n",
        "            loop_key, step_key = jax.random.split(loop_key)\n",
        "            # Embed the current tokens\n",
        "            decoder_input_embedded = model.embedding(current_tokens) # Shape: (B, 1, D)\n",
        "            if model.emb_dropout is not None:\n",
        "                decoder_input_embedded = model.emb_dropout(decoder_input_embedded)\n",
        "\n",
        "            # Pass through the decoder.\n",
        "            # `causal_mask=None` because MHA's `decode=True` mode handles causality internally using its cache index.\n",
        "            # The `encoder_attention_mask` is still needed for the cross-attention sub-layers.\n",
        "            decoder_output_step = model.decoder(\n",
        "                x=decoder_input_embedded, # Current token(s) embedding: (B, 1, D)\n",
        "                encoder_output=encoder_output,\n",
        "                causal_mask=None, # For decoder's self-attention\n",
        "                encoder_attention_mask=source_padding_mask, # For decoder's cross-attention\n",
        "            )\n",
        "\n",
        "            # Get logits from the final linear layer\n",
        "            logits_step = model.final_linear(decoder_output_step)  # Shape: (B, 1, VocabSize)\n",
        "\n",
        "            # Greedy sampling: select the token with the highest logit\n",
        "            # Logits are for the current position, so index with [:, -1, :] or [:, 0, :] if seq_len is 1\n",
        "            next_token_ids = gen_next_token_fn(logits_step[:, 0, :], step_key)  # Shape: (B,)\n",
        "\n",
        "            # For sequences that have already finished, keep inserting PAD_ID\n",
        "            next_token_ids = jnp.where(done_flags, pad_id, next_token_ids)\n",
        "\n",
        "            # Store the newly generated token\n",
        "            # step + 1 because generated_tokens[0] is BOS\n",
        "            generated_tokens = generated_tokens.at[:, step + 1].set(next_token_ids)\n",
        "\n",
        "            # Update flags for sequences that just generated EOS_ID\n",
        "            done_flags = jnp.logical_or(done_flags, (next_token_ids == eos_id))\n",
        "\n",
        "            # Prepare the input for the next decoding step\n",
        "            current_tokens = next_token_ids[:, jnp.newaxis]  # Shape: (B, 1)\n",
        "\n",
        "        return generated_tokens\n",
        "\n",
        "    def sample(self, source_token_ids: jax.Array) -> jax.Array:\n",
        "        \"\"\"\n",
        "        Public method to generate sequences.\n",
        "        Handles cache initialization/reset before calling the JITted generation loop.\n",
        "        \"\"\"\n",
        "        if not isinstance(source_token_ids, jax.Array):\n",
        "            source_token_ids = jnp.asarray(source_token_ids)\n",
        "\n",
        "        self._initialize_and_reset_caches(source_token_ids.shape[0], self.model)\n",
        "        loop_key = jax.random.key(0)\n",
        "        # Call the JIT-compiled generation function\n",
        "        return self.jitted_sample_fn(self.max_output_len, self.bos_id, self.eos_id, self.pad_id, self.model, source_token_ids, loop_key)"
      ],
      "metadata": {
        "id": "xS5KeQYXrP_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create the Sampler instance\n",
        "sampler = Sampler(\n",
        "    model=transformer_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_output_len=MAX_SEQ_LEN, # Or your desired max length\n",
        ")\n",
        "\n",
        "def decode_tokens(token_ids, tokenizer: spm.SentencePieceProcessor = tokenizer):\n",
        "  filter_set = set([tokenizer.bos_id(), tokenizer.eos_id(), tokenizer.pad_id()])\n",
        "  cleaned_ids = [int(id) for id in token_ids if int(id) not in filter_set]\n",
        "  return tokenizer.decode_ids(cleaned_ids)"
      ],
      "metadata": {
        "id": "030r-QdtKeih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the Sampler for Translation\n",
        "\n",
        "Now that our `Sampler` is built and configured with KV caching, we can use it to translate new English sentences that the model hasn't explicitly seen during training. To do this, we need to replicate the preprocessing steps applied to the training data: tokenize the input English sentence using our `sentencepiece` model, add the BOS and EOS tokens, and pad the sequence to `MAX_SEQ_LEN`.\n",
        "\n",
        "We'll create a function that takes a list of raw English strings, processes them into the token ID format expected by the `Sampler`, calls the `sampler.sample()` method, and then decodes the resulting token IDs back into human-readable Spanish strings."
      ],
      "metadata": {
        "id": "q3vF65PEKQ1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentences(\n",
        "    english_sentences: List[str],\n",
        "    sampler: Sampler, # Our sampler instance\n",
        "    tokenizer: spm.SentencePieceProcessor, # Our tokenizer\n",
        "    max_seq_len: int = MAX_SEQ_LEN, # Needs to match training/padding max length\n",
        "    bos_id: int = BOS_ID,\n",
        "    eos_id: int = EOS_ID,\n",
        "    pad_id: int = PAD_ID,\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Translates a list of English sentences into Spanish using the trained model.\n",
        "\n",
        "    Args:\n",
        "        english_sentences: A list of strings, each an English sentence to translate.\n",
        "        sampler: The Sampler instance configured for inference.\n",
        "        tokenizer: The SentencePiece tokenizer used by the model.\n",
        "        max_seq_len: The maximum sequence length the model was trained on.\n",
        "        bos_id: The token ID for the beginning of a sentence.\n",
        "        eos_id: The token ID for the end of a sentence.\n",
        "        pad_id: The token ID used for padding.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, each a translated Spanish sentence.\n",
        "    \"\"\"\n",
        "    processed_token_ids = []\n",
        "\n",
        "    # Preprocess each sentence: tokenize, add BOS/EOS, truncate, pad\n",
        "    for sentence in english_sentences:\n",
        "        # Tokenize the raw sentence\n",
        "        ids_raw = tokenizer.encode_as_ids(sentence)\n",
        "\n",
        "        # Truncate raw tokens to make space for BOS and EOS if needed\n",
        "        # The ProcessSentencePair truncated to max_seq_len - 2 BEFORE adding BOS/EOS\n",
        "        truncated_ids = ids_raw[:max_seq_len - 2]\n",
        "\n",
        "        # Add BOS and EOS tokens\n",
        "        ids_with_special = [bos_id] + truncated_ids + [eos_id]\n",
        "\n",
        "        # Pad the sequence to max_seq_len\n",
        "        current_len = len(ids_with_special)\n",
        "        padding_amount = max_seq_len - current_len\n",
        "\n",
        "        if padding_amount < 0:\n",
        "             # This case should ideally not happen if truncation to max_seq_len - 2 was correct\n",
        "             # but added as a safeguard. It means the input preprocessing is inconsistent\n",
        "             # with the model's expected max_seq_len.\n",
        "             print(f\"Warning: Processed sequence length {current_len} exceeds max_seq_len {max_seq_len} before padding. Truncating.\")\n",
        "             ids_with_special = ids_with_special[:max_seq_len]\n",
        "             current_len = max_seq_len\n",
        "             padding_amount = 0\n",
        "\n",
        "\n",
        "        padded_ids = np.pad(\n",
        "            ids_with_special,\n",
        "            pad_width=((0, padding_amount)),\n",
        "            mode='constant',\n",
        "            constant_values=pad_id\n",
        "        )\n",
        "        processed_token_ids.append(padded_ids)\n",
        "\n",
        "    # Stack processed IDs into a single NumPy array\n",
        "    input_token_array = np.stack(processed_token_ids, axis=0)\n",
        "\n",
        "    # Convert to JAX array (Sampler expects JAX array)\n",
        "    input_token_array_jax = jnp.asarray(input_token_array)\n",
        "\n",
        "    # Generate translations using the sampler\n",
        "    generated_token_ids_jax = sampler.sample(input_token_array_jax)\n",
        "\n",
        "    # Convert generated token IDs back to strings\n",
        "    translated_sentences = [\n",
        "        decode_tokens(tokens.tolist()) # Convert JAX array row back to list\n",
        "        for tokens in generated_token_ids_jax\n",
        "    ]\n",
        "\n",
        "    return translated_sentences\n",
        "\n",
        "# Example Usage:\n",
        "input_sentences = [\n",
        "    \"How are you?\",\n",
        "    \"My name is John.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"I like machine learning.\",\n",
        "    \"Hello world.\"\n",
        "]\n",
        "\n",
        "\n",
        "translated_outputs = translate_sentences(\n",
        "    input_sentences,\n",
        "    sampler=sampler,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        "    pad_id=PAD_ID,\n",
        "    bos_id=BOS_ID,\n",
        "    eos_id=EOS_ID\n",
        ")\n",
        "\n",
        "print(\"\\n--- Generated Translations ---\")\n",
        "for i, (input_sen, translated_sen) in enumerate(zip(input_sentences, translated_outputs)):\n",
        "    print(f\"Input ({i+1}): {input_sen}\")\n",
        "    print(f\"Output ({i+1}): {translated_sen}\")"
      ],
      "metadata": {
        "id": "AM2HS0FTyh_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¡Increíble! We Did It! (Or, At Least, We Built The Brains!)\n",
        "\n",
        "¡Felicidades! You've officially navigated the sometimes-treacherous, often rewarding path from zero to a working (albeit perhaps still learning) Transformer model capable of translating English to Spanish!\n",
        "\n",
        "Let's recap this epic quest:\n",
        "\n",
        "* We wrestled with dependencies and summoned the digital spirits of **JAX**, **Flax NNX**, **Optax**, **Orbax**, **Grain**, and **SentencePiece**.\n",
        "* We prepped our data like master chefs, downloading sentences and tokenizing them into digestible pieces.\n",
        "* We built a high-performance data pipeline using **Grain**, ensuring our model was never hungry.\n",
        "* We meticulously constructed the **Transformer architecture** from fundamental NNX modules: Positional Embeddings, Multi-Head Attention (using Flax's built-in), our own Feed-Forward Network, Encoder Layers, Decoder Layers, and finally, the complete Seq2Seq masterpiece.\n",
        "* We set up the training regimen with **Optax** for optimization, defined our loss function, and used **Orbax** for diligent checkpointing.\n",
        "* We bravely ran the **training loop**, watching the loss hopefully descend.\n",
        "* Finally, we crafted a **Sampler** specifically for inference, incorporating essential optimizations like KV caching, and saw our model produce its very first (or perhaps slightly garbled) Spanish translations!\n",
        "\n",
        "You've not only built a model but gained hands-on experience with the cutting-edge tools that power much of today's large-scale AI research and applications.\n",
        "\n",
        "**Where to Go From Here?**\n",
        "\n",
        "This is just the beginning! To turn our sassy translator into a true polyglot powerhouse, consider these next steps:\n",
        "\n",
        "* **Train for Longer:** The current `num_epochs` is quite small. Train the `config_big` model for significantly more epochs (e.g., 50, 100, or more) or on a larger dataset. This is crucial for better performance.\n",
        "* **Hyperparameter Tuning:** Experiment with different learning rates, dropout rates, hidden dimensions, and numbers of layers.\n",
        "* **Quantitative Evaluation:** Implement and calculate the **BLEU score** using `metrax` to get an objective measure of translation quality on a held-out test set.\n",
        "* **Advanced Decoding:** Explore more sophisticated sampling strategies like **Beam Search** for potentially higher quality output sequences compared to simple greedy or top-k sampling.\n",
        "* **Larger Models & Datasets:** Scale up to larger Transformer configurations and more extensive translation datasets.\n",
        "* **Deployment:** Learn how to save your model's state and load it for inference in a production environment.\n",
        "\n",
        "You now have the foundational knowledge and a working example to dive deeper into the fascinating world of sequence-to-sequence modeling and large language models.\n",
        "\n",
        "¡Adiós y buena suerte en tus futuras aventuras de código! (Goodbye and good luck on your future coding adventures!)"
      ],
      "metadata": {
        "id": "EC9sAVHZMSOO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dfnO6RYquEUt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}